---
title: DQN从入门到放弃2-增强学习与MDP
brief: 增强学习与MDP
key: 'DQN'
tags:
  - 机器学习
toc: true
comments: true
date: 2019-08-15 11:50:25
---

![](/pic/11/bg.jpg)
增强学习与MDP

<!-- more -->

## 1 上文回顾

在上一篇文章[DQN 从入门到放弃 第一篇](https://zhuanlan.zhihu.com/p/21262246?refer=intelligentunit)中，我们回答了三个问题：

*   为什么研究深度增强学习？
*   为什么研究DQN？
*   什么是增强学习？

那么在这一篇文章中，我们将进一步探讨增强学习的世界观以及随之而来的MDP马尔科夫决策过程。

## 2 增强学习的世界观

可能很少有人用世界观来看增强学习甚至人工智能的一些问题，但实际上任何问题的建立都是在一个基本的假设下进行构建的，也就是这个领域的世界观。

**那么，增强学习建立在怎样的世界观上呢？**

增强学习的研究依然建立在经典物理学的范畴上，也就是没有量子计算也没有相对论。这个世界的时间是可以分割成一个一个时间片的，并且有完全的先后顺序，因此可以形成

$$ \lbrace s_0,a_0,r_0,s_1,a_1,r_1,...s_t,a_t,r_t \rbrace $$

这样的状态，动作和反馈系列。这些数据样本是进行增强学习的基础。

另一个很重要的假设就是

**上帝不掷筛子！**

在增强学习的世界，我们相信如果输入是确定的，那么输出也一定是确定的。试想一下，有一个机械臂在练习掷筛子，以掷出6点作为目标。但是如果无论机械臂如何调整其关节的角度及扭矩，掷出的点数永远是随机的，那么无论如何也不可能通过算法使机械臂达成目标。因此，增强学习算法要有用，就是相信在增强学习中每一次参数的调整都会对世界造成确定性的影响。

以上两点便是增强学习算法建立的基础。当然了，基本上人工智能的研究都是在这样的基础上进行研究，探讨其世界观的意义并不是很大，但是意识到这一点可以有助于理解当前人工智能研究的局限性。

那么有了时间和确定性的假设，MDP（Markov Decision Process）便是为了描述这个世界而提出的概念。

## 3 MDP（Markov Decision Process）马尔科夫决策过程

MDP基于这样一种假设：
> 未来只取决于当前

什么意思呢？

就是如果我们站在上帝视角下看，我们知道这个世界的每个物体的状态，那么未来的变化只跟当前的状态相关，和过去没有关系。

用数学的话来描述就是：

一个状态$ S_t $是Markov当且仅当

$$ P(s_{t+1}|s_t)=P(s_{t+1}|s_t,s_{t-1},...s_1,s_0) $$

P为概率。简单的说就是下一个状态仅取决于当前的状态和当前的动作。注意**这里的状态是完全可观察的全部的环境状态（也就是上帝视角）**。当然，对于一些游戏比如围棋在游戏的世界中就是完全可观察的。上面的公式可以用概率论的方法来证明。这里，我们说一个更通俗易懂的方式：举个栗子，在理想环境中有一个球，以某一个速度v斜向上45度抛出，受重力G影响，求这个球的运行轨迹？初中的物理题。显然，我们知道了球的初速度，受力情况，我们可以完全计算出球在每一个时间点上位置和速度。而且我们只要知道某一个时间点上球的速度和受力情况，下一个时间点的速度和位置就可以求出。这就是一个MDP过程。

往大的说，如果宇宙起始于大爆炸的奇点。那么奇点状态确定，如果上帝不掷筛子的话，那么宇宙就是一个MDP的过程，它的每一步都是确定的，推到人身上就是每个人的命运也都是确定的。换句话讲，我们现在的所思所想不过是由我们身上的每个细胞导致的精神状态，意识以及周围的环境所完全确定的。所以MDP的宇宙大概没什么意思，还好还有量子力学和相对论，我也相信这个世界是不确定的。

回到增强学习的范畴，增强学习的问题都可以模型化为MDP的问题。

一个基本的MDP可以用（S,A,P）来表示，S表示状态，A表示动作，P表示状态转移概率，也就是根据当前的状态$ s_t $和$ a_t $转移到$ s_{t+1} $的概率。如果我们知道了转移概率P，也就是称为我们获得了**模型Model**，有了模型，未来就可以求解，那么获取最优的动作也就有可能，这种通过模型来获取最优动作的方法也就称为Model-based的方法。但是现实情况下，很多问题是很难得到准确的模型的，因此就有Model-free的方法来寻找最优的动作。关于具体的方法这里不具体讨论。在以后的文章中我们会通过分析具体的算法对此有个明确的认识。

## 4 回报Result

既然一个状态对应一个动作，或者动作的概率，而有了动作，下一个状态也就确定了。这就意味着每个状态可以用一个确定的值来进行描述。可以由此判断一个状态是好的状态还是不好的状态。比如，向左边走就是悬崖，悬崖肯定不是好的状态，再走一步可能就挂了，而向右走就是黄金，那么右边的状态就是好的状态。

那么状态的好坏其实等价于对未来回报的期望。因此，引入**回报Return**来表示某个时刻t的状态将具备的回报：
$$ G_t=R_{t+1}+\lambda R_{t+2}+...=\sum_{k=0}^\infty\lambda^kR_{t+k+1} $$

上面R是Reward反馈，λ是discount factor折扣因子，一般小于1，就是说一般当下的反馈是比较重要的，时间越久，影响越小。

那么实际上除非整个过程结束，否则显然我们无法获取所有的reward来计算出每个状态的Return，因此，再引入一个概念价值函数Value Function,用value function $ v(s) $来表示一个状态未来的潜在价值。还是上面的例子，这里就变成是向左看感觉左边是悬崖那么左边的状态的估值就低。

从定义上看，value function就是回报的期望：
$$ v(s)=\mathbb E[G_t|S_t=s] $$

引出价值函数，对于获取最优的策略Policy这个目标，我们就会有两种方法：

*   直接优化策略$ \pi(a|s) $或者$ a=\pi(s) $使得回报更高
*   通过估计value function来间接获得优化的策略。道理很简单，既然我知道每一种状态的优劣，那么我就知道我应该怎么选择了，而这种选择就是我们想要的策略。

当然了，还有第三种做法就是融合上面的两种做法，这也就是以后会讲到的actor-critic算法。但是现在为了理解DQN，我们将只关注第二种做法，就是估计value function的做法，因为DQN就是基于value function的算法。

## 5 What's Next?

有了价值函数value function，那么下一步我们就是需要考虑如何计算价值函数的问题了，这将引入一个极其重要的方程------Bellman方程，敬请期待下一篇文章的讲解。

## 链接
[DQN 从入门到放弃1 DQN与增强学习](https://zhuanlan.zhihu.com/p/21262246?refer=intelligentunit)
[DQN 从入门到放弃2 增强学习与MDP](https://zhuanlan.zhihu.com/p/21292697?refer=intelligentunit)

<span style="font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;background-color: rgb(255, 255, 255);letter-spacing: 0.5px;font-size: 14px;color: rgb(136, 136, 136);">
来源：

[Flood Sung](https://www.zhihu.com/people/flood-sung/activities)
[智能单元](https://zhuanlan.zhihu.com/intelligentunit)

</span>


