[{"title":"DQN从入门到放弃5-深度解读DQN算法","date":"2019-08-22T03:01:56.000Z","path":"2019/08/22/15-DQN从入门到放弃5-深度解读DQN算法/","text":"深度解读DQN算法 0 前言如果说DQN从入门到放弃的前四篇是开胃菜的话，那么本篇文章就是主菜了。所以，等吃完主菜再放弃吧！ 1 详解Q-Learning在上一篇文章DQN从入门到放弃 第四篇中，我们分析了动态规划Dynamic Programming并且由此引出了Q-Learning算法。可能一些知友不是特别理解。那么这里我们再用简单的语言描述一下整个思路是什么。 为了得到最优策略Policy，我们考虑估算每一个状态下每一种选择的价值Value有多大。然后我们通过分析发现，每一个时间片的Q(s,a)和当前得到的Reward以及下一个时间片的Q(s,a)有关。有些知友想不通，在一个实验里，我们只可能知道当前的Q值，怎么知道下一个时刻的Q值呢？大家要记住这一点，Q-Learning建立在虚拟环境下无限次的实验。这意味着可以把上一次实验计算得到的Q值拿来使用呀。这样，不就可以根据当前的Reward及上一次实验中下一个时间片的Q值更新当前的Q值了吗？说起来真是很拗口。下面用比较形象的方法再具体分析一下Q-Learning。 Q-Learning的算法如下： 对于Q-Learning，首先就是要确定如何存储Q值，最简单的想法就是用矩阵，一个s一个a对应一个Q值，所以可以把Q值想象为一个很大的表格，横列代表s，纵列代表a，里面的数字代表Q值，如下表示：这样大家就很清楚Q值是怎样的 了。接下来就是看如何反复实验更新。 Step 1：初始化Q矩阵，比如都设置为0 Step 2：开始实验。根据当前Q矩阵及$ \\epsilon-greedy $方法获取动作。比如当前处在状态s1，那么在s1一列每一个Q值都是0，那么这个时候随便选择都可以。假设我们选择a2动作，然后得到的reward是1，并且进入到s3状态，接下来我们要根据$$ Q(S_{t},A_{t}) \\leftarrow Q(S_{t},A_{t})+\\alpha({R_{t+1}+\\lambda \\max_aQ(S_{t+1},a)}-Q(S_t,A_t)) $$ 来更新Q值，这里我们假设$ \\alpha $是1，$ \\lambda $也等于1，也就是每一次都把目标Q值赋给Q。那么这里公式变成：$$ Q(S_t,A_t)=R_{t+1}+\\max_a Q(S_{t+1},a) $$ 所以在这里，就是$$ Q(s_1,a_2)=1+\\max_a Q(s_3,a) $$ 那么对应的s3状态，最大值是0，所以$ Q(s_1,a_2)=1+0=1 $,Q表格就变成：Step 3：接下来就是进入下一次动作，这次的状态是s3，假设选择动作a3，然后得到1的reward，状态变成s1，那么我们同样进行更新：$$ Q(s_3,a_3)=2+\\max_a Q(s_1,a)=2+1=3 $$ 所以Q的表格就变成： Step 4： 反复上面的方法。 就是这样，Q值在试验的同时反复更新。直到收敛。 相信这次知友们可以很清楚Q-Learning的方法了。接下来，我们将Q-Learning拓展至DQN。 2 维度灾难在上面的简单分析中，我们使用表格来表示Q(s,a)，但是这个在现实的很多问题上是几乎不可行的，因为状态实在是太多。使用表格的方式根本存不下。 举Atari为例子。计算机玩Atari游戏的要求是输入原始图像数据，也就是210x160像素的图片，然后输出几个按键动作。总之就是和人类的要求一样，纯视觉输入，然后让计算机自己玩游戏。那么这种情况下，到底有多少种状态呢？有可能每一秒钟的状态都不一样。因为，从理论上看，如果每一个像素都有256种选择，那么就有：$ 256^{210\\times 160} $ 这简直是天文数字。所以，我们是不可能通过表格来存储状态的。我们有必要对状态的维度进行压缩，解决办法就是 价值函数近似Value Function Approximation 3 价值函数近似Value Function Approximation什么是价值函数近似呢？说起来很简单，就是用一个函数来表示Q(s,a)。即$$ Q(s,a)=f(s,a) $$ f可以是任意类型的函数，比如线性函数：Q(s,a)=w_1s+w_2a+b 其中$ w_1,w_2,b $是函数f的参数。 大家看到了没有，通过函数表示，我们就可以无所谓s到底是多大的维度，反正最后都通过矩阵运算降维输出为单值的Q。 这就是价值函数近似的基本思路。 如果我们就用$ w $来统一表示函数f的参数，那么就有$$ Q(s,a)=f(s,a,w) $$ 为什么叫近似，因为我们并不知道Q值的实际分布情况，本质上就是用一个函数来近似Q值的分布，所以，也可以说是$$ Q(s,a)\\approx f(s,a,w) $$ 4 高维状态输入，低维动作输出的表示问题对于Atari游戏而言，这是一个高维状态输入（原始图像），低维动作输出（只有几个离散的动作，比如上下左右）。那么怎么来表示这个函数f呢？ 难道把高维s和低维a加在一起作为输入吗？ 必须承认这样也是可以的。但总感觉有点别扭。特别是，其实我们只需要对高维状态进行降维，而不需要对动作也进行降维处理。 那么，有什么更好的表示方法吗？ 当然有，怎么做呢？ 其实就是$ Q(s) \\approx f(s,w) $，只把状态s作为输入，但是输出的时候输出每一个动作的Q值，也就是输出一个向量$ [Q(s,a_1),Q(s,a_2),Q(s,a_3),…,Q(s,a_n)] $，记住这里输出是一个值，只不过是包含了所有动作的Q值的向量而已。这样我们就只要输入状态s，而且还同时可以得到所有的动作Q值，也将更方便的进行Q-Learning中动作的选择与Q值更新（这一点后面大家会理解）。 5 Q值神经网络化！终于到了和深度学习相结合的一步了！ 意思很清楚，就是我们用一个深度神经网络来表示这个函数f。 这里假设大家对深度学习特别是卷积神经网络已经有基本的理解。如果不是很清楚，欢迎阅读本专栏的CS231n翻译系列文章。以DQN为例，输入是经过处理的4个连续的84x84图像，然后经过两个卷积层，两个全连接层，最后输出包含每一个动作Q值的向量。 对于这个网络的结构，针对不同的问题可以有不同的设置。如果大家熟悉Tensorflow，那么肯定知道创建一个网络是多么简单的一件事。这里我们就不具体介绍了。我们将在之后的DQN tensorflow实战篇进行讲解。 总之，用神经网络来表示Q值非常简单，Q值也就是变成用Q网络（Q-Network）来表示。接下来就到了很多人都会困惑的问题，那就是 *怎么训练Q网络？？？* 6 DQN算法我们知道，神经网络的训练是一个最优化问题，最优化一个损失函数loss function，也就是标签和网络输出的偏差，目标是让损失函数最小化。为此，我们需要有样本，巨量的有标签数据，然后通过反向传播使用梯度下降的方法来更新神经网络的参数。 所以，要训练Q网络，我们要能够为Q网络提供有标签的样本。 所以，问题变成： *如何为Q网络提供有标签的样本？* 答案就是利用Q-Learning算法。 大家回想一下Q-Learning算法，Q值的更新依靠什么？依靠的是利用Reward和Q计算出来的目标Q值：$$ R_{t+1}+\\lambda \\max_aQ(S_{t+1},a) $$ 因此，我们把目标Q值作为标签不就完了？我们的目标不就是让Q值趋近于目标Q值吗？ 因此，Q网络训练的损失函数就是上面公式是$ s^,a^ $即下一个状态和动作。这里用了David Silver的表示方式，看起来比较清晰。 既然确定了损失函数，也就是cost，确定了获取样本的方式。那么DQN的整个算法也就成型了！ 接下来就是具体如何训练的问题了！ 7 DQN训练我们这里分析第一个版本的DQN，也就是NIPS 2013提出的DQN。我们分析了这么久终于到现在放上了DQN算法，真是不容易。如果没有一定基础直接上算法还真是搞不明白。 具体的算法主要涉及到Experience Replay，也就是经验池的技巧，就是如何存储样本及采样问题。 由于玩Atari采集的样本是一个时间序列，样本之间具有连续性，如果每次得到样本就更新Q值，受样本分布影响，效果会不好。因此，一个很直接的想法就是把样本先存起来，然后随机采样如何？这就是Experience Replay的意思。按照脑科学的观点，人的大脑也具有这样的机制，就是在回忆中学习。 那么上面的算法看起来那么长，其实就是反复试验，然后存储数据。接下来数据存到一定程度，就每次随机采用数据，进行梯度下降！ 也就是 在DQN中增强学习Q-Learning算法和深度学习的SGD训练是同步进行的！ 通过Q-Learning获取无限量的训练样本，然后对神经网络进行训练。 样本的获取关键是计算y，也就是标签。 8 小结好了，说到这，DQN的基本思路就介绍完了，不知道大家理解得怎么样？在下一篇文章中，我们将分析DQN在这一年来的发展变化！感谢知友们的关注！ 文中图片引用自 [1] Mnih, Volodymyr, et al. “Playing atari with deep reinforcement learning.” arXiv preprint arXiv:1312.5602 (2013). [2] Mnih, Volodymyr, et al. “Human-level control through deep reinforcement learning.” Nature 518.7540 (2015): 529-533. 链接DQN 从入门到放弃1 DQN与增强学习DQN 从入门到放弃2 增强学习与MDPDQN 从入门到放弃3 价值函数与Bellman方程DQN 从入门到放弃4 动态规划与Q-LearningDQN 从入门到放弃5 深度解读DQN算法DQN 从入门到放弃6 DQN的各种改进DQN 从入门到放弃7 连续控制DQN算法-NAF 来源： Flood Sung智能单元","comments":true,"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://forvoyager.github.io/tags/机器学习/"}]},{"title":"DQN从入门到放弃4-动态规划与Q-Learning","date":"2019-08-20T07:25:39.000Z","path":"2019/08/20/14-DQN从入门到放弃4-动态规划与Q-Learning/","text":"价值函数与Bellman方程 1 上文回顾在上一篇文章DQN从入门到放弃 第三篇中，我们分析到了Bellman方程，其方程$$ v(s)=\\mathbb E[R_{t+1}+\\lambda v(S_{t+1})|S_t=s] $$ 极其简洁，透出的含义就是价值函数的计算可以通过迭代的方式来实现。接下来本文将介绍如何构建基于Bellman方程的算法及Q-Learning。首先介绍动作价值函数 2 Action-Value function 动作价值函数前面我们引出了价值函数，考虑到每个状态之后都有多种动作可以选择，每个动作之下的状态又多不一样，我们更关心在某个状态下的不同动作的价值。显然。如果知道了每个动作的价值，那么就可以选择价值最大的一个动作去执行了。这就是Action-Value function $ Q^\\pi(s,a) $。那么同样的道理，也是使用reward来表示，只是这里的reward和之前的reward不一样，这里是执行完动作action之后得到的reward，之前state对应的reward则是多种动作对应的reward的期望值。显然，动作之后的reward更容易理解。 那么，有了上面的定义，动作价值函数就为如下表示：\\begin{align} Q^\\pi(s,a)&amp;=\\mathbb E[r_{t+1}+\\lambda r_{t+2}+\\lambda ^2r_{t+3}+…+|s,a]\\\\ &amp;=\\mathbb E_{s^\\prime}[r \\lambda Q^\\pi(s^\\prime,a^\\prime)|s,a]\\end{align} 这里要说明的是动作价值函数的定义，加了$ \\pi $,也就是说是在策略下的动作价值。因为对于每一个动作而已，都需要由策略根据当前的状态生成，因此必须有策略的支撑。而前面的价值函数则不一定依赖于策略。当然，如果定义$ v^\\pi(s) $则表示在策略$ \\pi $下的价值。 那么事实上我们会更多的使用动作价值函数而不是价值函数，因为动作价值函数更直观，更方便应用于算法当中。 3 Optimal value function 最优价值函数能计算动作价值函数是不够的，因为我们需要的是最优策略，现在求解最优策略等价于求解最优的value function，找到了最优的value function，自然而然策略也就是找到。（当然，这只是求解最优策略的一种方法，也就是value-based approach，由于DQN就是value-based，因此这里只讲这部分，以后我们会看到还有policy-based和model-based方法。一个就是直接计算策略函数，一个是估计模型，也就是计算出状态转移函数，从而整个MDP过程得解） 这里以动作价值函数来分析。 首先是最优动作价值函数和一般的动作价值函数的关系：$$ Q^*(s,a)=\\max_\\pi+Q^\\pi(s,a) $$ 也就是最优的动作价值函数就是所有策略下的动作价值函数的最大值。通过这样的定义就可以使最优的动作价值的唯一性，从而可以求解整个MDP。这部分在上一篇文章的评论中有介绍。 那么套用上一节得到的value function，可以得到$$ Q^(s,a)=\\mathbb E_{s^\\prime}[r+\\lambda+\\max+_{a^\\prime}Q^(s^\\prime,a^\\prime)|s,a] $$ 因为最优的Q值必然为最大值，所以，等式右侧的Q值必然为使a′取最大的Q值。 下面介绍基于Bellman方程的两个最基本的算法，策略迭代和值迭代。 4 策略迭代Policy IterationPolicy Iteration的目的是通过迭代计算value function 价值函数的方式来使policy收敛到最优。 Policy Iteration本质上就是直接使用Bellman方程而得到的：$$\\begin{aligned}v_{k+1}(s) &amp; \\doteq \\mathbb{E}{\\pi}\\left[R{t+1}+\\gamma v_{k}\\left(S_{t+1}\\right) | S_{t}=s\\right] \\\\&amp;=\\sum_{a} \\pi(a | s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{k}\\left(s^{\\prime}\\right)\\right]\\end{aligned}$$ 那么Policy Iteration一般分成两步： Policy Evaluation 策略评估。目的是 更新Value Function，或者说更好的估计基于当前策略的价值 Policy Improvement 策略改进。 使用 greedy policy 产生新的样本用于第一步的策略评估。本质上就是使用当前策略产生新的样本，然后使用新的样本更好的估计策略的价值，然后利用策略的价值更新策略，然后不断反复。理论可以证明最终策略将收敛到最优。具体算法： 5 Value Iteration 价值迭代Value Iteration则是使用Bellman 最优方程得到$$\\begin{aligned}v_{}(s) &amp;=\\max {a} \\mathbb{E}\\left[R{t+1}+\\gamma v_{}\\left(S_{t+1}\\right) | S_{t}=s, A_{t}=a\\right] \\\\ &amp;=\\max {a} \\sum{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{*}\\left(s^{\\prime}\\right)\\right]\\end{aligned}$$ 然后改变成迭代形式$$\\begin{aligned}v_{}(s) &amp;=\\max {a} \\mathbb{E}\\left[R{t+1}+\\gamma v_{}\\left(S_{t+1}\\right) | S_{t}=s, A_{t}=a\\right] \\\\ &amp;=\\max {a} \\sum{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{*}\\left(s^{\\prime}\\right)\\right]\\end{aligned}$$ value iteration的算法如下： 那么问题来了： Policy Iteration和Value Iteration有什么本质区别？ 为什么一个叫policy iteration，一个叫value iteration呢？ 原因其实很好理解，policy iteration使用bellman方程来更新value，最后收敛的value 即$ v_\\pi $是当前policy下的value值（所以叫做对policy进行评估），目的是为了后面的policy improvement得到新的policy。 而value iteration是使用bellman 最优方程来更新value，最后收敛得到的value即$ v_* $就是当前state状态下的最优的value值。因此，只要最后收敛，那么最优的policy也就得到的。因此这个方法是基于更新value的，所以叫value iteration。 从上面的分析看，value iteration较之policy iteration更直接。不过问题也都是一样，需要知道状态转移函数p才能计算。本质上依赖于模型，而且理想条件下需要遍历所有的状态，这在稍微复杂一点的问题上就基本不可能了。 那么上面引用的是价值函数的版本，那么如果是使用动作价值函数呢，公式基本是一样的：$$ Q_{i+1}(s,a)=\\mathbb E_{s^\\prime}[r+\\lambda+\\max_{a^\\prime}Q_i(s^\\prime,a^\\prime)|s,a] $$ 怎么直观的理解呢？ 我们还在计算当前的Q值，怎么能有下一个Q值呢？没有错。所以，我们只能用之前的Q值。也就是没次根据新得到的reward和原来的Q值来更新现在的Q值。理论上可以证明这样的value iteration能够使Q值收敛到最优的action-value function。 6 Q-LearningQ Learning的思想完全根据value iteration得到。但要明确一点是value iteration每次都对所有的Q值更新一遍，也就是所有的状态和动作。但事实上在实际情况下我们没办法遍历所有的状态，还有所有的动作，我们只能得到有限的系列样本。因此，只能使用有限的样本进行操作。那么，怎么处理？Q Learning提出了一种更新Q值的办法：$$ Q(S_{t},A_{t})+\\leftarrow+Q(S_{t},A_{t})+\\alpha({R_{t+1}+\\lambda+\\max_aQ(S_{t+1},a)}+-+Q(S_t,A_t)) $$ 虽然根据value iteration计算出target Q值，但是这里并没有直接将这个Q值（是估计值）直接赋予新的Q，而是采用渐进的方式类似梯度下降，朝target迈近一小步，取决于α,这就能够减少估计误差造成的影响。类似随机梯度下降，最后可以收敛到最优的Q值。 具体的算法如下： 7 Exploration and Exploitation 探索与利用在上面的算法中，我们可以看到需要使用某一个policy来生成动作，也就是说这个policy不是优化的那个policy，所以Q-Learning算法叫做Off-policy的算法。另一方面，因为Q-Learning完全不考虑model模型也就是环境的具体情况，只考虑看到的环境及reward，因此是model-free的方法。 回到policy的问题，那么要选择怎样的policy来生成action呢？有两种做法： 随机的生成一个动作 根据当前的Q值计算出一个最优的动作，这个policy$ \\pi $称之为greedy policy贪婪策略。也就是$$ \\pi(S_{t+1})+=+arg\\max+aQ(S{t+1},a) $$ 使用随机的动作就是exploration，也就是探索未知的动作会产生的效果，有利于更新Q值，获得更好的policy。而使用greedy policy也就是target policy则是exploitation，利用policy，这个相对来说就不好更新出更好的Q值，但可以得到更好的测试效果用于判断算法是否有效。 将两者结合起来就是所谓的$ \\epsilon-greedy $策略，$ \\epsilon $一般是一个很小的值，作为选取随机动作的概率值。可以更改$ \\epsilon $的值从而得到不同的exploration和exploitation的比例。 这里需要说明的一点是使用$ \\epsilon-greedy $策略是一种极其简单粗暴的方法，对于一些复杂的任务采用这种方法来探索未知空间是不可取的。因此，最近有越来越多的方法来改进这种探索机制。 8 小结到了这里，我们已经分析了Q-Learning算法，这也就是DQN所依赖的增强学习算法。下一步我们就讲直接分析DQN的算法实现了。 本文主要参考： 1 Reinforcement Learning: An Introduction2 Reinforcement Learning Course by David Silver 图片引用自： Reinforcement Learning Course by David Silver 的ppt 链接DQN 从入门到放弃1 DQN与增强学习DQN 从入门到放弃2 增强学习与MDPDQN 从入门到放弃3 价值函数与Bellman方程DQN 从入门到放弃4 动态规划与Q-LearningDQN 从入门到放弃5 深度解读DQN算法DQN 从入门到放弃6 DQN的各种改进DQN 从入门到放弃7 连续控制DQN算法-NAF 来源： Flood Sung智能单元","comments":true,"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://forvoyager.github.io/tags/机器学习/"}]},{"title":"DQN从入门到放弃3-价值函数与Bellman方程","date":"2019-08-16T05:54:22.000Z","path":"2019/08/16/13-DQN从入门到放弃3-价值函数与Bellman方程/","text":"价值函数与Bellman方程 1 上文回顾在上一篇文章DQN 从入门到放弃 第二篇中，我们探讨了增强学习问题的基本假设，然后引出了MDP马尔科夫决策过程。MDP只需要用一句话就可以说明白，就是“未来只取决于当前”，专业点说就是下一步的状态只取决于当前的状态，与过去的状态没有关系。这里大家要注意这里所说的状态是完全可观察的，也就是上帝眼中的世界。再举例说明一下完全可观察的意思就是比如我们的眼睛看到的世界，那就是不完全可观察的，我们并不清楚的知道眼前的每一个物体，比如人，车，动物的真实物理位置，因此也就是无法准确知道它们下一个时刻的状态（比如车的位置）只能通过估算的方法来估计。而在上帝眼中，那么每一个物体的位置和速度信息都是确定的，也因此下一个时刻的状态也就是完全确定的。 在引出了MDP之后，由于每一个时刻的状态是确定的，因此我们可以用Value Function价值函数来描述这个状态的价值，从而确定我们的决策方式。有知友表示不是很理解Value Function，那么下面我们再具体探讨一下。 2 Value Function 价值函数我们用一个例子来说明Value Function的含义与重要性。 这是一个投资决策问题：假如我们有一笔X美刀的资金，我们眼前有三种选择来使用这笔资金： 使用资金进行股票投资 使用资金进行买房投资 使用资金购买书籍，科研设备等提升资金 那么，我们就面临如何做选择的问题。这里假设我们只能选择其中的一个做选择。 我们先来解释一下直接基于Policy的方法是怎样的。 直接基于Policy的意思就是我们有一套Policy策略，我们基于这个策略进行操作，比如可以有如下所示的策略： if 资金X &gt; 500000: &emsp;&emsp;选择股票投资 else if 资金X &gt; 100000: &emsp;&emsp;选择房产投资 else: &emsp;&emsp;选择买书，买设备自我提升 那么上面的伪代码就是表示一个极其简单的策略。这个策略只考虑资金量，输入资金量，输出决策方式。如果把Policy策略看做是一个黑箱，那么基于策略的方法就是：那么如果不是基于Policy策略直接做出决策，我们还有什么办法呢？ 显然有，而且大家可以从上面的简单策略看到一个最大的缺陷，就是上面的策略完全不考虑每一种选择未来的价值。我们做决策是有目的的，那就是为了最大化未来的回报Result是不是？那么对于上面的投资选择问题，我们的目标就是希望我们的投资回报率最高。因此，上面的简单策略竟然完全不考虑每一种选择的价值，而仅考虑资金量，显然是一种欠考虑的方法。因此，我们是不是应该评估一下每一种选择的潜在价值呢？耶，价值Value出来了，是不是？通过对价值的评估，我们也就可以有如下的做决策的方法：我们就评估每一种状态（选择+资金量）的价值，然后选择价值最高的作为最后的决策。 比如说： if 投资股市: &emsp;&emsp;因为股市低迷，价值为-100 if 投资房产 + 资金量 &gt; 100000: &emsp;&emsp;因为房产泡沫还没破，各地房价还在涨，价值为+500 if 提升自己 + 资金量 &lt; 100000: &emsp;&emsp;当前人工智能潜力巨大，资金又不算太大，投资自己价值为+1000 …(更多的评估价值的方法） OK, 假设现在我们有50000的资金，那么根据我们的价值估算方法，选择投资自己的价值最大，为+1000，因此我们就选择投资自己作为决策结果。 从数学的角度，我们常常会用一个函数$ V(s) $来表示一个状态的价值，也可以用$ Q(s,a) $来表示状态及某一个动作的价值。我们上面的例子就是来评估某一个状态下动作的价值，然后根据价值做出判断。实际上我们这里也是有策略的，我们的策略更简单： if 某一个决策的价值最大：&emsp;&emsp;选择这个决策 这就是价值函数的意义。在后面的文章当中，我们还会发现，其实我们还可以同时使用策略加价值评估的方法来联合给出决策，这种算法就是所谓的Actor-Critic算法。这里就不多加介绍了。 3 再谈增强学习的意义从上面的例子想必大家会发现，增强学习面向的决策与控制问题与我们的行为息息相关。我们想要让计算机自己能够学习做出某种决策，并且是可以不断改进的决策。对于人工智能，最大的目的不就是要创造智能，会自己思考，能模仿人类的行为，从而能够代替人类做事情吗？增强学习的目的就是希望计算机能够模仿人类的行为。这样一看，知友们是不是马上觉得增强学习意义非常之大。 实际上增强学习的算法都是基于人类的行为而构建的。比如上面的Value Function价值函数，实际上人类自己做决策的时候也就是那么做的。这里只不过是把它数学化了而已。 按照DeepMind的David Silver，也就是AlphaGo的第一作者所言： DL + RL = AI DL深度学习给了计算机“神经网络大脑”，RL给了计算机学习机制。这两者结合起来，就能创造智能！ 多说这么几句只是想让知友们真正意识到 Deep Reinforcement Learning深度增强学习为什么这么重要，这么值得研究。 接下来，我们介绍Bellman方程，增强学习领域最重要的一个方程。很多算法都是基于Bellman方程衍生而来。 4 Bellman方程在上文我们介绍了Value Function价值函数，所以为了解决增强学习的问题，一个显而易见的做法就是—- 我们需要估算Value Function 是的，只要我们能够计算出价值函数，那么最优决策也就得到了。因此，问题就变成了如何计算Value Function？ 怎么估算价值呢？ 我们还是先想想我们人是怎么估算的？我们还是以上面的投资决策问题来作为例子 一般我们基于以下几种情况来做评估： 其他人的选择。比如有很多人投资股市失败，因此我们就会降低我们投资股票的价值。 自己的反复试验。我们常常不是只做一次选择，而是做了很多次选择，从而收获了所谓的“经验”的东西。我们根据经验来评估选择的价值。比如我们做了好几次投资楼市的选择，结果大获成功，因此我们就会认为投资楼市是不错的选择。 基于理性分析。我们根据我们已有的知识对当前的情况做分析，从而做出一定的判断。 基于感性的逻辑。比如选择投资自己到人工智能领域。虽然我们大约觉得人工智能前景很好，但是我们真正要投资自己到这个领域有时候仅仅是出于一种热爱或者说一种理想主义。就是不管别人觉得好不好，反正我觉得好，我就这么选了。 计算机要如何才能评估价值呢？ 其他人的选择。不好意思，计算机只有自己，没有其他人。也许你会说多台计算机。如果是共用一个“大脑”做分布式计算，那还是只有自己。 基于理性分析。不好意思，计算机在面对问题时往往什么都不知道，比如基于屏幕玩Atari游戏，计算机压根不知道看到的像素是个什么东西。它没有人类的先验知识，无法分析。（当然啦，先使用监督学习然后再增强学习的AlphaGo就是有先验知识的情况下做的增强学习） 基于感性的逻辑。不好意思，计算机目前还产生不了感性。 那么，基于自己的反复试验呢？耶，这个可以啊。计算机这方面比人类强多了，可以24小时不分昼夜的反复试验，然后对价值做出正确的判断。 所以，Value Function从分析上是可以评估出来的，那具体该怎么评估呢？ 我们下面将不得不引入点数学公式，虽然也会非常好理解。 还记得回报Result的基本定义吗？就是所有Reward的累加（带衰减系数discount factor）$$ G_t=R_{t+1}+\\lambda R_{t+2}+…=\\sum_{k=0}^\\infty\\lambda^kR_{t+k+1} $$ 那么Value Function该如何定义？也很简单，就是期望的回报啊！期望的回报越高，价值显然也就越大，也就越值得去选择。用数学来定义就是如下：$$ v(s)=\\mathbb E[G_t|S_t=s] $$ 接下来，我们把上式展开如下：\\begin{align}v(s)&amp;=\\mathbb E[G_t|S_t=s]\\\\ &amp;=\\mathbb E[R_{t+1}+\\lambda R_{t+2}+\\lambda ^2R_{t+3}+…|S_t=s]\\\\ &amp;=\\mathbb E[R_{t+1}+\\lambda (R_{t+2}+\\lambda R_{t+3}+…)|S_t=s]\\\\ &amp;=\\mathbb E[R_{t+1}+\\lambda+G_{t+1}|S_t=s]\\\\ &amp;=\\mathbb E[R_{t+1}+\\lambda+v(S_{t+1})|S_t=s]\\end{align} 因此，$$ v(s)=\\mathbb E[R_{t+1}+\\lambda v(S_{t+1})|S_t=s] $$ 上面这个公式就是Bellman方程的基本形态。从公式上看，当前状态的价值和下一步的价值以及当前的反馈Reward有关。 它表明Value Function是可以通过迭代来进行计算的!!! 5 What’s Next?Bellman方程是这么简洁的一个等式，但却是增强学习算法的基础。在下一篇文章中，我们将探讨Dynamic Programming动态规划，也就是基于Bellman方程而衍生得到的求解Value Function的方法。敬请关注。 链接DQN 从入门到放弃1 DQN与增强学习DQN 从入门到放弃2 增强学习与MDPDQN 从入门到放弃3 价值函数与Bellman方程DQN 从入门到放弃4 动态规划与Q-LearningDQN 从入门到放弃5 深度解读DQN算法DQN 从入门到放弃6 DQN的各种改进DQN 从入门到放弃7 连续控制DQN算法-NAF 来源： Flood Sung智能单元","comments":true,"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://forvoyager.github.io/tags/机器学习/"}]},{"title":"DQN从入门到放弃2-增强学习与MDP","date":"2019-08-15T03:50:25.000Z","path":"2019/08/15/12-DQN从入门到放弃2-增强学习与MDP/","text":"增强学习与MDP 1 上文回顾在上一篇文章DQN 从入门到放弃 第一篇中，我们回答了三个问题： 为什么研究深度增强学习？ 为什么研究DQN？ 什么是增强学习？ 那么在这一篇文章中，我们将进一步探讨增强学习的世界观以及随之而来的MDP马尔科夫决策过程。 2 增强学习的世界观可能很少有人用世界观来看增强学习甚至人工智能的一些问题，但实际上任何问题的建立都是在一个基本的假设下进行构建的，也就是这个领域的世界观。 那么，增强学习建立在怎样的世界观上呢？ 增强学习的研究依然建立在经典物理学的范畴上，也就是没有量子计算也没有相对论。这个世界的时间是可以分割成一个一个时间片的，并且有完全的先后顺序，因此可以形成 $$ \\lbrace s_0,a_0,r_0,s_1,a_1,r_1,…s_t,a_t,r_t \\rbrace $$ 这样的状态，动作和反馈系列。这些数据样本是进行增强学习的基础。 另一个很重要的假设就是 上帝不掷筛子！ 在增强学习的世界，我们相信如果输入是确定的，那么输出也一定是确定的。试想一下，有一个机械臂在练习掷筛子，以掷出6点作为目标。但是如果无论机械臂如何调整其关节的角度及扭矩，掷出的点数永远是随机的，那么无论如何也不可能通过算法使机械臂达成目标。因此，增强学习算法要有用，就是相信在增强学习中每一次参数的调整都会对世界造成确定性的影响。 以上两点便是增强学习算法建立的基础。当然了，基本上人工智能的研究都是在这样的基础上进行研究，探讨其世界观的意义并不是很大，但是意识到这一点可以有助于理解当前人工智能研究的局限性。 那么有了时间和确定性的假设，MDP（Markov Decision Process）便是为了描述这个世界而提出的概念。 3 MDP（Markov Decision Process）马尔科夫决策过程MDP基于这样一种假设： 未来只取决于当前 什么意思呢？ 就是如果我们站在上帝视角下看，我们知道这个世界的每个物体的状态，那么未来的变化只跟当前的状态相关，和过去没有关系。 用数学的话来描述就是： 一个状态$ S_t $是Markov当且仅当 $$ P(s_{t+1}|s_t)=P(s_{t+1}|s_t,s_{t-1},…s_1,s_0) $$ P为概率。简单的说就是下一个状态仅取决于当前的状态和当前的动作。注意这里的状态是完全可观察的全部的环境状态（也就是上帝视角）。当然，对于一些游戏比如围棋在游戏的世界中就是完全可观察的。上面的公式可以用概率论的方法来证明。这里，我们说一个更通俗易懂的方式：举个栗子，在理想环境中有一个球，以某一个速度v斜向上45度抛出，受重力G影响，求这个球的运行轨迹？初中的物理题。显然，我们知道了球的初速度，受力情况，我们可以完全计算出球在每一个时间点上位置和速度。而且我们只要知道某一个时间点上球的速度和受力情况，下一个时间点的速度和位置就可以求出。这就是一个MDP过程。 往大的说，如果宇宙起始于大爆炸的奇点。那么奇点状态确定，如果上帝不掷筛子的话，那么宇宙就是一个MDP的过程，它的每一步都是确定的，推到人身上就是每个人的命运也都是确定的。换句话讲，我们现在的所思所想不过是由我们身上的每个细胞导致的精神状态，意识以及周围的环境所完全确定的。所以MDP的宇宙大概没什么意思，还好还有量子力学和相对论，我也相信这个世界是不确定的。 回到增强学习的范畴，增强学习的问题都可以模型化为MDP的问题。 一个基本的MDP可以用（S,A,P）来表示，S表示状态，A表示动作，P表示状态转移概率，也就是根据当前的状态$ s_t $和$ a_t $转移到$ s_{t+1} $的概率。如果我们知道了转移概率P，也就是称为我们获得了模型Model，有了模型，未来就可以求解，那么获取最优的动作也就有可能，这种通过模型来获取最优动作的方法也就称为Model-based的方法。但是现实情况下，很多问题是很难得到准确的模型的，因此就有Model-free的方法来寻找最优的动作。关于具体的方法这里不具体讨论。在以后的文章中我们会通过分析具体的算法对此有个明确的认识。 4 回报Result既然一个状态对应一个动作，或者动作的概率，而有了动作，下一个状态也就确定了。这就意味着每个状态可以用一个确定的值来进行描述。可以由此判断一个状态是好的状态还是不好的状态。比如，向左边走就是悬崖，悬崖肯定不是好的状态，再走一步可能就挂了，而向右走就是黄金，那么右边的状态就是好的状态。 那么状态的好坏其实等价于对未来回报的期望。因此，引入回报Return来表示某个时刻t的状态将具备的回报：$$ G_t=R_{t+1}+\\lambda R_{t+2}+…=\\sum_{k=0}^\\infty\\lambda^kR_{t+k+1} $$ 上面R是Reward反馈，λ是discount factor折扣因子，一般小于1，就是说一般当下的反馈是比较重要的，时间越久，影响越小。 那么实际上除非整个过程结束，否则显然我们无法获取所有的reward来计算出每个状态的Return，因此，再引入一个概念价值函数Value Function,用value function $ v(s) $来表示一个状态未来的潜在价值。还是上面的例子，这里就变成是向左看感觉左边是悬崖那么左边的状态的估值就低。 从定义上看，value function就是回报的期望：$$ v(s)=\\mathbb E[G_t|S_t=s] $$ 引出价值函数，对于获取最优的策略Policy这个目标，我们就会有两种方法： 直接优化策略$ \\pi(a|s) $或者$ a=\\pi(s) $使得回报更高 通过估计value function来间接获得优化的策略。道理很简单，既然我知道每一种状态的优劣，那么我就知道我应该怎么选择了，而这种选择就是我们想要的策略。 当然了，还有第三种做法就是融合上面的两种做法，这也就是以后会讲到的actor-critic算法。但是现在为了理解DQN，我们将只关注第二种做法，就是估计value function的做法，因为DQN就是基于value function的算法。 5 What’s Next?有了价值函数value function，那么下一步我们就是需要考虑如何计算价值函数的问题了，这将引入一个极其重要的方程——Bellman方程，敬请期待下一篇文章的讲解。 链接DQN 从入门到放弃1 DQN与增强学习DQN 从入门到放弃2 增强学习与MDPDQN 从入门到放弃3 价值函数与Bellman方程DQN 从入门到放弃4 动态规划与Q-LearningDQN 从入门到放弃5 深度解读DQN算法DQN 从入门到放弃6 DQN的各种改进DQN 从入门到放弃7 连续控制DQN算法-NAF 来源： Flood Sung智能单元","comments":true,"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://forvoyager.github.io/tags/机器学习/"}]},{"title":"DQN从入门到放弃1-DQN与增强学习","date":"2019-08-14T03:14:48.000Z","path":"2019/08/14/11-DQN从入门到放弃1-DQN与增强学习/","text":"DQN与增强学习 1 前言深度增强学习Deep Reinforcement Learning是将深度学习与增强学习结合起来从而实现从Perception感知到Action动作的端对端学习End-to-End Learning的一种全新的算法。简单的说，就是和人类一样，输入感知信息比如视觉，然后通过深度神经网络，直接输出动作，中间没有hand-crafted engineering的工作。深度增强学习具备使机器人实现真正完全自主的学习一种甚至多种技能的潜力。 虽然将深度学习和增强学习结合的想法在几年前就有人尝试，但真正成功的开端就是DeepMind在NIPS 2013上发表的Playing Atari with Deep Reinforcement Learning一文，在该文中第一次提出Deep Reinforcement Learning 这个名称，并且提出DQN（Deep Q-Network）算法，实现从纯图像输入完全通过学习来玩Atari游戏的成果。之后DeepMind在Nature上发表了改进版的DQN文章Human-level Control through Deep Reinforcement Learning，引起了广泛的关注，Deep Reinfocement Learning 从此成为深度学习领域的前沿研究方向。 而Hinton，Bengio及Lecun三位大神在Nature上发表的Deep Learning综述一文最后也将Deep Reinforcement Learning作为未来Deep Learning的发展方向。引用一下原文的说法： We expect much of thefuture progress in vision to come from systems that are trained end-to-endand combine ConvNets with RNNs that use reinforcement learningto decide where to look. 从上面的原文可见三位大神对于Deep Reinforcement Learning的期待。而显然这一年来的发展没有让大家失望，AlphaGo横空出世，将进一步推动Deep Reinforcement Learning的发展。 Deep Reinforcement Learning的重要关键在于其具备真正实现AI的潜力，它使得计算机能够完全通过自学来掌握一项任务，甚至超过人类的水平。也因此，DeepMind很早受到了Google等企业的关注。DeepMind 50多人的团队在2014年就被Google以4亿美元的价格收购。而15年12月份刚刚由Elon Musk牵头成立的OpenAI，则一开始就获得了10亿美元的投资，而OpenAI中的好几位成员都来自UC Berkerley的Pieter Abbeel团队。 Pieter Abbeel团队紧随DeepMind之后，采用基于引导式监督学习直接实现了机器人的End-to-End学习，其成果也引起了大量的媒体报道和广泛关注。去年的NIPS 2015 更是由Pieter Abbeel及DeepMind的David Silver联合组织了Deep Reinforcement Learning workshop。可以说，目前在Deep Reinforcement Learning取得开拓性进展的主要集中在DeepMind和UC Berkerley团队。 为了研究Deep Reinforcement Learning，DQN的学习是首当其冲的。只有真正理解了DQN算法，才能说对Deep Reinforcement Learning入门。要理解并掌握DQN算法，需要增强学习和深度学习的多方面知识，笔者在2014年底开始接触DQN，但由于对基础知识掌握不全，导致竟然花了近1年的时间才真正理解DQN的整个算法。因此，本专栏从今天开始推出 DQN 从入门到放弃 系列文章，意在通过对增强学习，深度学习等基础知识的讲解，以及基于Tensorflow的代码实现，使大家能够扎实地从零开始理解DQN，入门Deep Reinforcement Learning。本系列文章将以一周一篇的速度更新。另外要说明的一点是DQN已被Google申请专利，因此只能做研究用，不能商用。 2 预备条件虽然说是从零开始，但是DQN毕竟也还属于深度学习领域的前沿算法，为了理解本系列的文章，知友们还是需要有一定的基础： 一定的概率论和线性代数基础（数学基础） 一定的Python编程基础（编程基础，后面的代码实现将完全基于Tensorflow实现） 考虑到目前理解深度学习的知友肯定比理解增强学习的知友多，并且专栏也在同步翻译CS231N的内容，本系列文章计划用极短的篇幅来介绍DQN所使用的深度学习知识，而用更多的篇幅介绍增强学习的知识。 如果知友们具备以上的基本预备条件，那么我们就可以开始DQN学习之旅了。 接下来本文将介绍增强学习的基础知识。 3 增强学习是什么在人工智能领域，一般用智能体Agent来表示一个具备行为能力的物体，比如机器人，无人车，人等等。那么增强学习考虑的问题就是智能体Agent和环境Environment之间交互的任务。比如一个机械臂要拿起一个手机，那么机械臂周围的物体包括手机就是环境，机械臂通过外部的比如摄像头来感知环境，然后机械臂需要输出动作来实现拿起手机这个任务。再举玩游戏的例子，比如我们玩极品飞车游戏，我们只看到屏幕，这就是环境，然后我们输出动作（键盘操作）来控制车的运动。 那么，不管是什么样的任务，都包含了一系列的动作Action,观察Observation还有反馈值Reward。所谓的Reward就是Agent执行了动作与环境进行交互后，环境会发生变化，变化的好与坏就用Reward来表示。如上面的例子。如果机械臂离手机变近了，那么Reward就应该是正的，如果玩赛车游戏赛车越来越偏离跑道，那么Reward就是负的。接下来这里用了Observation观察一词而不是环境那是因为Agent不一定能得到环境的所有信息，比如机械臂上的摄像头就只能得到某个特定角度的画面。因此，只能用Observation来表示Agent获取的感知信息。 那么知道了整个过程，任务的目标就出来了，那就是要能获取尽可能多的Reward。没有目标，控制也就无从谈起，因此，获取Reward就是一个量化的标准，Reward越多，就表示执行得越好。每个时间片，Agent都是根据当前的观察来确定下一步的动作。观察Observation的集合就作为Agent的所处的状态State，因此，状态State和动作Action存在映射关系，也就是一个state可以对应一个action，或者对应不同动作的概率（常常用概率来表示，概率最高的就是最值得执行的动作）。状态与动作的关系其实就是输入与输出的关系，而状态State到动作Action的过程就称之为一个策略Policy，一般用表示，也就是需要找到以下关系： $$ a=\\pi(s) $$或者$$ \\pi(a|s) $$ 其中a是action，s是state。第一种是一一对应的表示，第二种是概率的表示。 增强学习的任务就是找到一个最优的策略Policy从而使Reward最多。 我们一开始并不知道最优的策略是什么，因此往往从随机的策略开始，使用随机的策略进行试验，就可以得到一系列的状态,动作和反馈： $$ \\lbrace s_1,a_1,r_1,s_2,a_2,r_2,…s_t,a_t,r_t \\rbrace $$ 这就是一系列的样本Sample。增强学习的算法就是需要根据这些样本来改进Policy，从而使得得到的样本中的Reward更好。由于这种让Reward越来越好的特性，所以这种算法就叫做增强学习Reinforcement Learning。 4 What’s Next?在下一篇文章中，笔者将和大家分享MDP马尔科夫决策过程的知识，这是构建增强学习算法的基础。敬请关注！ 链接DQN 从入门到放弃1 DQN与增强学习DQN 从入门到放弃2 增强学习与MDPDQN 从入门到放弃3 价值函数与Bellman方程DQN 从入门到放弃4 动态规划与Q-LearningDQN 从入门到放弃5 深度解读DQN算法DQN 从入门到放弃6 DQN的各种改进DQN 从入门到放弃7 连续控制DQN算法-NAF 来源： Flood Sung智能单元","comments":true,"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://forvoyager.github.io/tags/机器学习/"}]},{"title":"10.机器学习概览","date":"2019-07-22T02:02:17.000Z","path":"2019/07/22/10-机器学习概览/","text":"机器学习的来龙去脉…… 机器学习的定义Wikipedia有言： 1**Machine learning (ML)** is the scientific study of algorithms and statistical models that computer systems use in order to perform a specific task effectively without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. 大意：机器学习（ML）是对计算机系统使用的算法和统计模型的科学研究，以便有效地执行特定任务而不使用明确的指令，而是依赖于模式和推理。它被视为人工智能的一个子集。 百度百科 1机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。 总的来说，就是： 机器学习的历史详情请参考链接3 1980 年代 主导流派：符号主义 架构：服务器或大型机 主导理论：知识工程 基本决策逻辑：决策支持系统，实用性有限 1990 年代到 2000 年 主导流派：贝叶斯 架构：小型服务器集群 主导理论：概率论 分类：可扩展的比较或对比，对许多任务都足够好了 2010 年代早期到中期 主导流派：联结主义 架构：大型服务器农场 主导理论：神经科学和概率 识别：更加精准的图像和声音识别、翻译、情绪分析等 2010 年代末期 主导流派：联结主义+符号主义 架构：许多云 主导理论：记忆神经网络、大规模集成、基于知识的推理 简单的问答：范围狭窄的、领域特定的知识共享 2020 年代+ 主导流派：联结主义+符号主义+贝叶斯+…… 架构：云计算和雾计算 主导理论：感知的时候有网络，推理和工作的时候有规则 简单感知、推理和行动：有限制的自动化或人机交互 2040 年代+ 主导流派：算法融合 架构：无处不在的服务器 主导理论：最佳组合的元学习 感知和响应：基于通过多种学习方式获得的知识或经验采取行动或做出回答 机器学习的分类慢慢体会…… 监督学习根据已知的输入和输出训练模型，让模型能够预测未来输出。 监督式机器学习能够根据已有的包含不确定性的数据建立一个预测模型。监督式学习算法接受已知的输入数据集（包含预测变量）和对该数据集的已知响应（输出，响应变量），然后训练模型，使模型能够对新输入数据的响应做出合理的预测。如果您尝试去预测已知数据的输出，则使用监督式学习。 监督式学习采用分类和回归技术开发预测模型。 具体算法的适用范围 回归回归技术可预测连续的响应。例如，温度的变化或电力需求中的波动。典型的应用包括电力系统负荷预测和算法交易。 常用的回归算法主要有：线性模型、非线性模型、规则化、逐步回归、提升 (boosted) 和袋装 (bagged) 决策树、神经网络和自适应神经模糊学习。 分类分类可预测离散的响应。例如，电子邮件是不是垃圾邮件，肿瘤是恶性还是良性的。分类模型可将输入数据划分成不同类别。典型的应用包括医学成像、语音识别和信用评估。 常用的分类算法主要有：支持向量机 (SVM)、提升 (boosted) 决策树和袋装 (bagged) 决策树、k-最近邻、朴素贝叶斯 (Naïve Bayes)、判别分析、逻辑回归和神经网络。 无监督学习无监督学习可发现数据中隐藏的模式或内在结构。这种技术可根据未做标记的输入数据集得到推论。 机器学习应用 计算金融学，用于信用评估和算法交易 图像处理和计算机视觉，用于人脸识别、运动检测和对象检测 计算生物学，用于肿瘤检测、药物发现和 DNA 序列分析 能源生产，用于预测价格和负载 汽车、航空航天和制造业，用于预见性维护 自然语言处理，用于语音识别应用 链接 http://usblogs.pwc.com/emerging-technology/a-look-at-machine-learning-infographic/ http://usblogs.pwc.com/emerging-technology/machine-learning-methods-infographic/ http://usblogs.pwc.com/emerging-technology/machine-learning-evolution-infographic/","comments":true,"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://forvoyager.github.io/tags/机器学习/"}]},{"title":"9.如何找到最耗时的java线程？","date":"2019-07-19T05:32:16.000Z","path":"2019/07/19/9-如何找到最耗时的java线程？/","text":"如何找到最耗时的java线程？ 找到JAVA进程pid得先找到目标进程 123ps -ef|grep java或者jps -mlv 结果如下 1234root 102945 100545 0 13:44 pts/0 00:00:00 grep --color=auto javaroot 123398 1 0 7月18 ? 00:04:26 java -jar ./jar/p-discovery.jarroot 123584 1 0 7月18 ? 00:03:33 java -jar ./jar/p-config-service.jarroot 124627 1 1 7月18 ? 00:20:17 java -jar ./jar/p-invest-service.jar 第二列就是pid 找到线程中最耗时的线程TID命令如下 1top -Hp pid 本例如下 123456789101112131415161718top -Hp 123584结果如下：top - 13:49:20 up 158 days, 1:52, 2 users, load average: 0.34, 0.20, 0.16Threads: 63 total, 0 running, 63 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.4 us, 1.0 sy, 0.0 ni, 98.4 id, 0.2 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 32528944 total, 252228 free, 28587204 used, 3689512 buff/cacheKiB Swap: 16777212 total, 14890172 free, 1887040 used. 3287160 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND123584 root 20 0 13.583g 1.136g 13268 S 0.0 3.7 0:00.00 java123601 root 20 0 13.583g 1.136g 13268 S 0.0 3.7 0:21.79 java123602 root 20 0 13.583g 1.136g 13268 S 0.0 3.7 0:23.72 java123605 root 20 0 13.583g 1.136g 13268 S 0.0 3.7 0:38.36 java123626 root 20 0 13.583g 1.136g 13268 S 0.0 3.7 0:01.59 java123627 root 20 0 13.583g 1.136g 13268 S 0.0 3.7 0:00.00 java123629 root 20 0 13.583g 1.136g 13268 S 0.0 3.7 0:18.94 java123684 root 20 0 13.583g 1.136g 13268 S 0.0 3.7 0:00.39 java 找到最耗时的tid，转成16进制，如上面的123605耗时比较大 123printf &quot;%x\\n&quot; 123605结果为：1e2d5 打印线程栈信息得到上面的信息后，就可以找到目标：耗时长的线程。命令如下 1jstack [pid] | grep [tid16进制] 本例如下 1234jstack 123584 | grep 1e2d5结果如下：&quot;SimplePauseDetectorThread_0&quot; #20 daemon prio=9 os_prio=0 tid=0x00007f9cac059000 nid=0x1e2ed waiting on condition [0x00007f9cbc8c1000] 注实际过程中，这个命令需要多执行几次，观察栈的变化才能真正的确认问题。","comments":true,"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://forvoyager.github.io/tags/JAVA/"}]},{"title":"8.python性能分析之函数执行时间","date":"2019-07-18T02:51:04.000Z","path":"2019/07/18/8-python性能分析之函数执行时间/","text":"统计python执行过程中的耗时情况…… 生成性能分析文件用cProfile（Python默认的性能分析器）生成性能分析文件，命令格式如下： 1python -m cProfile -o result.cprofile your_python_file.py [parameters] 如下 1python -m cProfile -o result.cprofile exe_pre.py -r 600549 最后会在当前路径下生成一个result.cprofile的分析文件。 可视化分析文件可选的可视化工具比较多，如gprof2dot、Matlab、snakeviz等。以snakeviz为例： 安装snakeviz1pip install snakeviz 可视化1snakeviz result.cprofile 效果如图，风格自选：","comments":true,"tags":[{"name":"PYTHON","slug":"PYTHON","permalink":"http://forvoyager.github.io/tags/PYTHON/"}]},{"title":"7.gym学习及二次开发","date":"2019-07-05T03:23:05.000Z","path":"2019/07/05/7-gym学习及二次开发/","text":"工欲善其事必先利其器，借助一个好的开发平台，我们能够快速地将理论知识变成代码，从而实现和验证我们的一些想法。Openai的gym（openai/gym）便是这样一个好的平台。之所以说它好，是因为它背后有一支强大的团队在支持，维护和更新，这保证了平台的可持续性。而且它是openai于2017年5月16号，也就是前天释放出来的roboschool（openai/roboschool）的基础；它还可以与谷歌的tensorflow联合使用，等等。因此，我们在实战系列的第一讲中比较全面地介绍下gym。为方便交流，公布个qq交流群202570720。 本讲打算分3个小节对gym进行介绍。 第1小节：gym安装及简单的demo示例第2小节：深入剖析gym的环境构建第3小节：构建自己的gym环境 第1小节 gym安装及简单的demo示例该小节讲的是gym在ubuntu下的安装，其他系统可参考官网安装过程。 1.1 为了便于管理，需要先装anaconda具体下载和安装步骤如下：1.下载anaconda安装包，推荐利用清华镜像来下载，下载地址为：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive 。 我装的是Anaconda3-4.3.0版本。 2.安装anaconda。下载完成anaconda后，安装包会在Dowloads文件夹下，在终Ctrl+Alt+T打开终端）键入cd Downloads， 然后键入 bash Anaconda3_4.3.0-Linux-x86_64.sh(小技巧，键入bash an然后按Tab键，linux系统会自动补全后面的名字) 3.安装过程会询问你是否将路径安装到环境变量中，键入yes， 至此Anaconda安装完成。你会在目录/home/你的用户名文件夹下面看到anaconda3。关掉终端，再开一个，这样环境变量才起作用。 1.2 利用anaconda建一个虚拟环境Anaconda创建虚拟环境的格式为：conda create –-name 你要创建的名字 python=版本号。比如我创建的虚拟环境名字为gymlab(你可以用自己的环境名）, 用的python版本号为3.5，可这样写： 1conda create –-name gymlab python=3.5 操作完此步之后，会在anaconda3/envs文件夹下多一个gymlab。Python3.5就在gymlab下得lib文件夹中。 1.3 安装gym上一步已经装了一个虚拟环境gymlab, 在这一步要应用。 开一个新的终端，然后用命令source activate gymlab激活虚拟环境，然后再装gym。具体步骤如下： 1.键入git clone openai/gym，将gym克隆到计算机中，如果你的计算机中没有安装git, 那么可以键入： 1sudo apt install git 先安装git. 2.cd gym 进入gym文件夹 3.pip install –e ‘.[all]’进行完全安装。等待，会装一系列的库等，装完后可以将你的gym安装文件的目录写到环境变量中，一种方法是打开.bashrc文件，在末尾加入语句： 1export PYTHONPATH=你的gym目录：$PYTHONPATH。 如果不出意外的话，你就可以开始享用gym了。如果报错可以先安装依赖项，键入命令，然后再按照Step3的命令安装。 1sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig 下面给出一个最简单的例子1.开一个终端(ctr+alt+t)， 然后激活用anaconda建立的虚拟环境： 1source activate gymlab 2.运行python 1python 3.导入gym模块 1import gym 4.创建一个小车倒立摆模型 1env = gym.make(‘CartPole-v0’) 5.初始化环境 1env.reset() 6.刷新当前环境，并显示 1env.render() 通过这6步，我们可以看到一个小车倒立摆系统.如下图所示： 第2小节 深入剖析gym环境构建我们继续讲，从第1小节的尾巴开始。有三个重要的函数：env =gym.make(‘CartPole-v0’)env.reset()env.render() 第一个函数是创建环境，我们会在第3小节具体讲如何创建自己的环境，所以这个函数暂时不讲。第二个函数env.reset()和第三个函数env.render()是每个环境文件都包含的函数。我们以cartpole为例，对这两个函数进行讲解。 Cartpole的环境文件在~你的gym目录/gym/envs/classic_control/cartpole.py. 该文件定义了一个CartPoleEnv的环境类，该类的成员函数有：seed(),step(),reset()和render().第1小节调用的就是CartPoleEnv的两个成员函数reset()和render()。下面，我们先讲讲这两个函数，再介绍step()函数 2.1 reset()函数详解reset()为重新初始化函数。那么这个函数有什么用呢？ 在强化学习算法中，智能体需要一次次地尝试，累积经验，然后从经验中学到好的动作。一次尝试我们称之为一条轨迹或一个episode.每次尝试都要到达终止状态.一次尝试结束后，智能体需要从头开始，这就需要智能体具有重新初始化的功能。函数reset()就是这个作用。 reset()的源代码为： 1234def _reset()self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))self.steps_beyond_done = Nonereturn np.array(self.state) 第一行代码是利用均匀随机分布初试化环境的状态。第二行代码是设置当前步数为None第3行，返回环境的初始化状态。 2.2 render()函数详解render()函数在这里扮演图像引擎的角色。一个仿真环境必不可少的两部分是物理引擎和图像引擎。物理引擎模拟环境中物体的运动规律；图像引擎用来显示环境中的物体图像。其实，对于强化学习算法，该函数可以没有。但是，为了便于直观显示当前环境中物体的状态，图像引擎还是有必要的。另外，加入图像引擎可以方便我们调试代码。下面具体介绍gym如何利用图像引擎来创建图像。 我们直接看源代码： 12345678910111213141516171819def _render(self, mode=’human’, close=False):if close:#省略，直接看关键代码部分if self.viewer is None:from gym.envs.classic_controlimport rendering #这一句导入rendering模块，利用rendering模块中的画图函数进行图形的绘制#如绘制600*400的窗口函数为：self.viewer = rendering.Viewer(screen_width, screen_height)#其中screen_width=600，screen_height=400#创建小车的代码为：l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2axleoffset =cartheight/4.0cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)]) 其中rendering.FilledPolygon为填充一个矩形。 创建完cart的形状，接下来给cart添加平移属性和旋转属性。将车的位移设置到cart的平移属性中，cart就会根据系统的状态变化左右运动。具体代码解释，我已上传到github上面了，gxnk/reinforcement-learning-code 。想深入了解的同学可去下载学习。 2.3 step()函数详解该函数在仿真器中扮演物理引擎的角色。其输入是动作a，输出是：下一步状态，立即回报，是否终止，调试项。 该函数描述了智能体与环境交互的所有信息，是环境文件中最重要的函数。在该函数中，一般利用智能体的运动学模型和动力学模型计算下一步的状态和立即回报，并判断是否达到终止状态。 我们直接看源代码： 12345678910111213141516171819202122232425262728293031def _step(self, action):assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))state = self.statex, x_dot, theta, theta_dot = state #系统的当前状态force = self.force_mag if action==1 else -self.force_mag #输入动作，即作用到车上的力costheta = math.cos(theta) #余弦函数sintheta = math.sin(theta) #正弦函数#底下是车摆的动力学方程式，即加速度与动作之间的关系。temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_massthetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass)) #摆的角加速度xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass #小车的平移加速x = x + self.tau * x_dotx_dot = x_dot + self.tau * xacctheta = theta + self.tau * theta_dottheta_dot = theta_dot + self.tau * thetaacc #积分求下一步的状态self.state = (x,x_dot,theta,theta_dot) 2.4 一个简单的demo下面，我给出一个最简单的demo，让大家体会一下上面三个函数如何使用。 1234567891011121314import gymimport timeenv = gym.make('CartPole-v0') #创造环境observation = env.reset() #初始化环境，observation为环境状态count = 0for t in range(100): action = env.action_space.sample() #随机采样动作 observation, reward, done, info = env.step(action) #与环境交互，获得下一步的时刻 if done: break env.render() #绘制场景 count+=1 time.sleep(0.2) #每次等待0.2sprint(count) #打印该次尝试的步数 第3小节 创建自己的gym环境并利示例qlearning的方法在上一小节中以cartpole为例子深入剖析了gym环境文件的重要组成。我们知道，一个gym环境最少的组成需要包括reset()函数和step()函数。当然，图像显示函数render()一般也是需要的。这一节，我会以机器人找金币为例给大家演示如何构建一个全新的gym环境，并以此环境为例，示例最经典的强化学习算法qlearning算法。在3.1节中，给出机器人找金币的问题陈述；第3.2节中，给出构建gym环境的过程；第3.3节中，利用qlearning方法实现机器人找金币的智能决策。全部代码已传到github上。 3.1 机器人找金币的问题陈述 上为机器人在网格世界找金币的示意图。该网格世界一共有８个状态，其中状态６和状态8为死亡区域，状态７为金币区域。机器人的初始位置为网格世界中任意一个状态。机器人从初始状态出发寻找金币。机器人进行一次探索，进入死亡区域或找到金币，本次探测结束。机器人找到金币的回报为１，进入死亡区域回报为－１，机器人在区域１－５之间转换时，回报为０。我们的目标是找到一个策略使得机器人不管处在什么状态（１－５）都能找到金币。对于这个机器人找金币的游戏，我们可以利用强化学习的方法来实现。 3.2 构建网格世界的gym环境一个gym的环境文件，其主体是个类，在这里我们定义类名为：GridEnv, 其初始化为环境的基本参数，因为机器人找金币的过程是一个马尔科夫过程，我们在强化学习入门课程的第一讲已经介绍过了一个马尔科夫过程应该包括状态空间，动作空间，回报函数，状态转移概率。因此，我们在类GridEnv的初始化时便给出了相应的定义。网格世界的全部代码在gxnk/reinforcement-learning-code,文件名为 grid_mdp.py 我们看源代码： 状态空间为： 1self.states = [1,2,3,4,5,6,7,8] 动作空间为： 1self.actions = [**'n'**,**'e'**,**'s'**,**'w'**] 回报函数为： 1234self.rewards = dict(); #回报的数据结构为字典self.rewards['1_s'] = -1.0self.rewards['3_s'] = 1.0self.rewards['5_s'] = -1.0 状态转移概率为： 123456789101112self.t = dict(); #状态转移的数据格式为字典self.t[**'1_s'**] = 6self.t[**'1_e'**] = 2self.t[**'2_w'**] = 1self.t[**'2_e'**] = 3self.t[**'3_s'**] = 7self.t[**'3_w'**] = 2self.t[**'3_e'**] = 4self.t[**'4_w'**] = 3self.t[**'4_e'**] = 5self.t[**'5_s'**] = 8self.t[**'5_w'**] = 4 有了状态空间，动作空间和状态转移概率，我们便可以写step(a)函数了。这里特别注意的是，step()函数的输入是动作，输出为：下一个时刻的动作，回报，是否终止，调试信息。尤其需要注意的是输出的顺序不要弄错了。对于调试信息，可以为空，但不能缺少，否则会报错，常用{}来代替。我们看源代码： 12345678910111213141516171819202122232425def _step(self, action): #系统当前状态 state = self.state ＃判断系统当前状态是否为终止状态 if state in self.terminate_states: return state, 0, True, &#123;&#125; key = \"%d_%s\"%(state, action) #将状态和动作组成字典的键值 #状态转移 if key in self.t: next_state = self.t[key] else: next_state = state self.state = next_state is_terminal = False if next_state in self.terminate_states: is_terminal = True if key not in self.rewards: r = 0.0 else: r = self.rewards[key] return next_state, r,is_terminal,&#123;&#125; step()函数就是这么简单。下面我们重点介绍下如何写render()函数。从图1.1机器人找金币的示意图我们可以看到，网格世界是由一些线和圆组成的。因此，我们可以调用rendering中的画图函数来绘制这些图像。 整个图像是一个600*400的窗口，可用如下代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081from gym.envs.classic_control import renderingself.viewer = rendering.Viewer(screen_width, screen_height)# 创建网格世界，一共包括11条直线，事先算好每条直线的起点和终点坐标，然后绘制这些直线，代码如下：self.line1 = rendering.Line((100,300),(500,300))self.line2 = rendering.Line((100, 200), (500, 200))self.line3 = rendering.Line((100, 300), (100, 100))self.line4 = rendering.Line((180, 300), (180, 100))self.line5 = rendering.Line((260, 300), (260, 100))self.line6 = rendering.Line((340, 300), (340, 100))self.line7 = rendering.Line((420, 300), (420, 100))self.line8 = rendering.Line((500, 300), (500, 100))self.line9 = rendering.Line((100, 100), (180, 100))self.line10 = rendering.Line((260, 100), (340, 100))self.line11 = rendering.Line((420, 100), (500, 100))# 接下来，创建死亡区域，我们用黑色的圆圈代表死亡区域，源代码如下：# 创建第一个骷髅self.kulo1 = rendering.make_circle(40)self.circletrans = rendering.Transform(translation=(140,150))self.kulo1.add_attr(self.circletrans)self.kulo1.set_color(0,0,0)# 创建第二个骷髅self.kulo2 = rendering.make_circle(40)self.circletrans = rendering.Transform(translation=(460, 150))self.kulo2.add_attr(self.circletrans)self.kulo2.set_color(0, 0, 0)# 创建金币区域，用金色的圆来表示：# 创建金条self.gold = rendering.make_circle(40)self.circletrans = rendering.Transform(translation=(300, 150))self.gold.add_attr(self.circletrans)self.gold.set_color(1, 0.9, 0)# 创建机器人，我们依然用圆来表示机器人，为了跟死亡区域和金币区域不同，我们可以设置不同的颜色：# 创建机器人self.robot= rendering.make_circle(30)self.robotrans = rendering.Transform()self.robot.add_attr(self.robotrans)self.robot.set_color(0.8, 0.6, 0.4)# 创建完之后，给11条直线设置颜色，并将这些创建的对象添加到几何中代码如下：self.line1.set_color(0, 0, 0)self.line2.set_color(0, 0, 0)self.line3.set_color(0, 0, 0)self.line4.set_color(0, 0, 0)self.line5.set_color(0, 0, 0)self.line6.set_color(0, 0, 0)self.line7.set_color(0, 0, 0)self.line8.set_color(0, 0, 0)self.line9.set_color(0, 0, 0)self.line10.set_color(0, 0, 0)self.line11.set_color(0, 0, 0)self.viewer.add_geom(self.line1)self.viewer.add_geom(self.line2)self.viewer.add_geom(self.line3)self.viewer.add_geom(self.line4)self.viewer.add_geom(self.line5)self.viewer.add_geom(self.line6)self.viewer.add_geom(self.line7)self.viewer.add_geom(self.line8)self.viewer.add_geom(self.line9)self.viewer.add_geom(self.line10)self.viewer.add_geom(self.line11)self.viewer.add_geom(self.kulo1)self.viewer.add_geom(self.kulo2)self.viewer.add_geom(self.gold)self.viewer.add_geom(self.robot)# 接下来，开始设置机器人的位置。机器人的位置根据其当前所处的状态不同，所在的位置不同。我们事先计算出每个状态处机器人位置的中心坐标，并存储到两个向量中，并在类初始化中给出：self.x=[140,220,300,380,460,140,300,460]self.y=[250,250,250,250,250,150,150,150]# 根据这两个向量和机器人当前的状态，我们就可以设置机器人当前的圆心坐标了即：if self.state is None: return Noneself.robotrans.set_translation(self.x[self.state-1], self.y[self.state- 1])return self.viewer.render(return_rgb_array=mode == 'rgb_array') 以上便完成了render()函数的建立。 reset()函数的建立 reset()函数常常用随机的方法初始化机器人的状态，即： 123def _reset(self): self.state = self.states[int(random.random() * len(self.states))] return self.state 全部的代码请去github上下载学习。下面重点讲一讲如何将建好的环境进行注册，以便通过gym的标准形式进行调用。其实环境的注册很简单，只需要３步： 第一步：将我们自己的环境文件（我创建的文件名为grid_mdp.py)拷贝到你的gym安装目录/gym/gym/envs/classic_control文件夹中。（拷贝在这个文件夹中因为要使用rendering模块。当然，也有其他办法。该方法不唯一） 第二步：打开该文件夹（第一步中的文件夹）下的init.py文件，在文件末尾加入语句： 12from gym.envs.classic_control.grid_mdpimport GridEnv 第三步：进入文件夹你的gym安装目录/gym/gym/envs，打开该文件夹下的init.py文件，添加代码： 123456register(id='GridWorld-v0',entry_point='gym.envs.classic_control:GridEnv',max_episode_steps=200,reward_threshold=100.0,) 第一个参数id就是你调用gym.make(‘id’)时的id, 这个id你可以随便选取，我取的，名字是GridWorld-v0第二个参数就是函数路口了。 后面的参数原则上来说可以不必要写。经过以上三步，就完成了注册。下面，我们给个简单的demo来测试下我们的环境的效果吧： 我们依然写个终端程序： 12345source activate gymlab pythonenv = gym.make(**'GridWorld-v0'**)env.reset()env.render() 代码运行后会出现如图1.2所示的效果： 3.3 qlearning方法实现机器人找金币 在强化学习入门第四讲中强化学习入门 第四讲 时间差分法（TD方法） - 知乎专栏，已经给出了qlearning的伪代码，现在我们利用已经建立的环境用python对伪代码进行实现，源代码如下： 12345678910111213141516171819202122232425262728293031323334def qlearning(num_iter1, alpha, epsilon): x = [] y = [] qfunc = dict() #行为值函数为字典 #初始化行为值函数为0 for s in states: for a in actions: key = \"%d_%s\"%(s,a) qfunc[key] = 0.0 for iter1 in range(num_iter1): x.append(iter1) y.append(compute_error(qfunc)) #初始化初始状态 s = grid.reset() a = actions[int(random.random()*len(actions))] t = False count = 0 while False == t and count &lt;100: key = \"%d_%s\"%(s, a) #与环境进行一次交互，从环境中得到新的状态及回报 s1, r, t1, i =grid.step(a) key1 = \"\" #s1处的最大动作 a1 = greedy(qfunc, s1) key1 = \"%d_%s\"%(s1, a1) #利用qlearning方法更新值函数 qfunc[key] = qfunc[key] + alpha*(r + gamma * qfunc[key1]-qfunc[key]) #转到下一个状态 s = s1; a = epsilon_greedy(qfunc, s1, epsilon) count += 1 plt.plot(x,y,\"-.,\",label =\"q alpha=%2.1f epsilon=%2.1f\"%(alpha,epsilon))return qfunc qlearning是异策略算法，其行动策略为epsilon-greedy策略，而评估策略为greedy策略。在代码中的体现为：行动策略为： 1a = epsilon_greedy(qfunc, s1, epsilon) 评估策略为： 12345a1 = greedy(qfunc, s1)key1 = &quot;%d_%s&quot;%(s1, a1)#利用qlearning方法更新值函数qfunc[key] = qfunc[key] + alpha*(r + gamma * qfunc[key1]-qfunc[key]) 上面的qun[key1]就是评估的贪婪策略。 上面的qlearning算法中的greedy()和epsilon_greedy在文件中都有定义，大家可登录到github下载下来学习,该部分代码在gxnk/reinforcement-learning-code目录下文件名为qlearning.py中。 最后我们给出一个机器人找金币学习和策略的例子。 机器人利用qlearning的方法学习，然后打印学到的策略，再测试学到的策略。具体代码在gxnk/reinforcement-learning-code目录下learning_and_test文件中。 其他相关的代码都可在我的github上下载gxnk/reinforcement-learning-code 来源： 强化学习知识大讲堂","comments":true,"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://forvoyager.github.io/tags/机器学习/"}]},{"title":"6.Understanding LSTM Networks","date":"2019-07-03T01:16:55.000Z","path":"2019/07/03/6-Understanding-LSTM-Networks/","text":"Recurrent Neural NetworksHumans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence. Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones. Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist. Recurrent Neural Networks have loops. In the above diagram, a chunk of neural network, AAA, looks at some input xtxtx_t and outputs a value hthth_t. A loop allows information to be passed from one step of the network to the next. These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop: An unrolled recurrent neural network. This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data. And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning… The list goes on. I’ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy’s excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks. But they really are pretty amazing. Essential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore. The Problem of Long-Term DependenciesOne of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends. Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information. But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large. Unfortunately, as that gap grows, RNNs become unable to learn to connect the information. In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. Thankfully, LSTMs don’t have this problem! LSTM NetworksLong Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. The repeating module in a standard RNN contains a single layer. LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way. The repeating module in an LSTM contains four interacting layers. Don’t worry about the details of what’s going on. We’ll walk through the LSTM diagram step by step later. For now, let’s just try to get comfortable with the notation we’ll be using. In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations. &lt;!– To be a bit more explicit, we can split up each line into lines carrying individual scalar values: The Core Idea Behind LSTMsThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged. The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation. The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!” An LSTM has three of these gates, to protect and control the cell state. Step-by-Step LSTM Walk ThroughThe first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1ht−1h_{t-1} and xtxtx_t, and outputs a number between 000 and 111 for each number in the cell state Ct−1Ct−1C_{t-1}. A 111 represents “completely keep this” while a 000 represents “completely get rid of this.” Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject. The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, CtCt\\tilde{C}_t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state. In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting. It’s now time to update the old cell state, Ct−1Ct−1C_{t-1}, into the new cell state CtCtC_t. The previous steps already decided what to do, we just need to actually do it. We multiply the old state by ftftf_t, forgetting the things we decided to forget earlier. Then we add it∗Ctit∗Cti_t*\\tilde{C}_t. This is the new candidate values, scaled by how much we decided to update each state value. In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps. Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanhtanh\\tanh (to push the values to be between −1−1-1 and 111) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to. For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next. Variants on Long Short Term MemoryWhat I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them. One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state. The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others. Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015). There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014). Which of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that they’re all about the same. Jozefowicz, et al. (2015) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks. ConclusionEarlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks! Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable. LSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, Xu, et al. (2015) do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner… Attention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by Kalchbrenner, et al. (2015) seem extremely promising. Work using RNNs in generative models – such as Gregor, et al. (2015), Chung, et al. (2015), or Bayer &amp; Osendorfer (2015) – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so! AcknowledgmentsI’m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post. I’m very grateful to my colleagues at Google for their helpful feedback, especially Oriol Vinyals, Greg Corrado, Jon Shlens, Luke Vilnis, and Ilya Sutskever. I’m also thankful to many other friends and colleagues for taking the time to help me, including Dario Amodei, and Jacob Steinhardt. I’m especially thankful to Kyunghyun Cho for extremely thoughtful correspondence about my diagrams. Before this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback. In addition to the original authors, a lot of people contributed to the modern LSTM. A non-comprehensive list is: Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Graves.↩ More Posts Attention and Augmented Recurrent Neural Networks A Modular Perspective Neural Networks, Manifolds, and Topology Deep Learning, NLP, and Representations 来源： colah’s blogGithub","comments":true,"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://forvoyager.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"http://forvoyager.github.io/tags/算法/"}]},{"title":"5.如何设计高可用的分布式锁","date":"2019-06-27T03:32:00.000Z","path":"2019/06/27/5-如何设计高可用的分布式锁/","text":"分布式锁在分布式环境下，锁定全局唯一公共资源，表现为：请求串行化互斥性 第一步是上锁的资源目标，是锁定全局唯一公共资源，只有是全局唯一的资源才存在多个线程或服务竞争的情况。 互斥性表现为一个资源的隔离级别串行化，如果对照单机事务 ACID 的隔离性来说，互斥性的事务隔离级别是 SERLALIZABLE，属于最高的隔离级别。 事务隔离级别：DEFAULTREAD_UNCOMMITTEDREAD_COMMITEDREPEATABLE_READSERLALIZABLE 分布式锁目的分布式锁的目的如下： 解决业务层幂等性 解决 MQ 消费端多次接受同一消息 确保串行|隔离级别 多台机器同时执行定时任务 寻找唯一资源进行上锁例子： 防止用户重复下单 共享资源进行上锁的对象 ： 【用户id】 订单生成后发送MQ给消费者进行积分的添加 寻找上锁的对象 ：【订单id】 用户已经创建订单，准备对订单进行支付，同时商家在对这个订单进行改价 寻找上锁对象 ： 【订单id】 基于 Redis 分布式锁Redis 单线程串行处理天然就是解决串行化问题，用来解决分布式锁是再适合不过。 实现方式： 12setnx key value Expire_time 获取到锁 返回 1 ， 获取失败 返回 0 存在问题如下 锁时间不可控 Redis 只能在 Setnx 指定一个锁的超时时间，假设初始设定锁的时间是 10 秒钟，但是业务获取到锁跑了 20 秒钟，在 10 秒钟之后，如果又有一个业务可以获取到相同的一把锁。 这个时候可能就存在两个相同的业务都获取得到锁的问题，并且两个业务处在并行阶段。也就是第一个获取锁的业务无法对自身的锁进行续租。 单点连接超时问题 Redis 的 Client 与 Server 端并没有维持心跳的机制，如果在连接中出现问题，Client 会得到一个超时的回馈。 主从问题 Redis 的集群实际上在 CAP 模式中是处在与 AP 的模型，保证可用性。在主从复制中“主”有数据，但可能“从”还没有数据。这个时候，一旦主挂掉或者网络抖动等各种原因，可能会切换到“从”节点。 这个时候有可能会导致两个业务线程同时的获取到两把锁： ①业务线程-1：向主节点请求锁 ②业务线程-1：获取锁 ③业务线程-1：获取到锁并开始执行业务 ④这个时候 Redis 刚生成的锁在主从之间还未进行同步 ⑤Redis 这时候主节点挂掉了 ⑥Redis 的从节点升级为主节点 ⑦业务线程-2：向新的主节点请求锁 ⑧业务线程-2：获取到新的主节点返回的锁 ⑨业务线程-2：获取到锁开始执行业务 ⑩这个时候业务线程-1和业务线程-2同时在执行任务 Redlock 上述的问题其实并不是 Redis 的缺陷，只是 Redis 采用了 AP 模型，它本身无法确保我们对一致性的要求。 Redis 官方推荐 Redlock 算法来保证，问题是 Redlock 至少需要三个 Redis 主从实例来实现，维护成本比较高。 相当于 Redlock 使用三个 Redis 集群实现了自己的另一套一致性算法，比较繁琐，在业界也使用得比较少。 能不能使用 Redis 作为分布式锁能不能使用 Redis 作为分布式锁，这个本身就不是 Redis 的问题，还是取决于业务场景，我们先要自己确认我们的场景是适合 AP 还是 CP。 如果在社交发帖等场景下，我们并没有非常强的事务一致性问题，Redis 提供给我们高性能的 AP 模型是非常适合的。 但如果是交易类型，对数据一致性非常敏感的场景，我们可能要寻找一种更加适合的 CP 模型。 Redis 可能作为高可用的分布式锁并不合适，我们需要确立高可用分布式锁的设计目标。 高可用分布式锁设计目标高可用分布式锁的设计目标如下：强一致性，是 CP 模型服务高可用，不存在单点问题锁能够续租和自动释放业务接入简单 三种分布式锁方案对比常用的三种分布式锁方案对比如下图： 基于 Zookeeper 分布式锁刚刚也分析过，Redis 其实无法确保数据的一致性，先来看 Zookeeper 是否合适作为我们需要的分布式锁。 首先 ZK 的模式是 CP 模型，也就是说，当 ZK 锁提供给我们进行访问的时候，在 ZK 集群中能确保这把锁在 ZK 的每一个节点都存在。 这个实际上是 ZK 的 Leader 通过二阶段提交写请求来保证的，这个也是 ZK 的集群规模大了的一个瓶颈点。 ZK 锁实现的原理说 ZK 的锁问题之前先看看 Zookeeper 中的几个特性，这几个特性构建了 ZK 的一把分布式锁。 Zookeeper 中的几个特性如下： 有序节点，当在一个父目录下如 /lock 下创建有序节点，节点会按照严格的先后顺序创建出自节点 lock000001，lock000002，lock0000003，以此类推，有序节点能严格保证各个自节点按照排序命名生成。 临时节点，客户端建立了一个临时节点，在客户端的会话结束或会话超时，Zookeeper 会自动删除该节点 ID。 事件监听，在读取数据时，我们可以对节点设置监听，当节点的数据发生变化(1 节点创建，2 节点删除，3 节点数据变动，4 子节点变动)时，Zookeeper 会通知客户端。 结合这几个特点，来看下 ZK 是怎么组合分布式锁： 业务线程-1，业务线程-2 分别向 ZK 的 /lock 目录下，申请创建有序的临时节点。 业务线程-1 抢到 /lock0001 的文件，也就是在整个目录下最小序的节点，也就是线程-1 获取到了锁。 业务线程-2 只能抢到 /lock0002 的文件，并不是最小序的节点，线程 2 未能获取锁。 业务线程-1 与 lock0001 建立了连接，并维持了心跳，维持的心跳也就是这把锁的租期。 当业务线程-1 完成了业务，将释放掉与 ZK 的连接，也就是释放了这把锁。 ZK 分布式锁的代码实现ZK 官方提供的客户端并不支持分布式锁的直接实现，我们需要自己写代码去利用 ZK 的这几个特性去进行实现： ZK 分布式锁客户端假死的问题客户端创建了临时有序节点并建立了事件监听，就可以让业务线程与 ZK 维持心跳，这个心跳也就是这把锁的租期。 当客户端的业务线程完成了执行就把节点进行删除，也就释放了这把锁，不过中间也可能存在问题： 客户端挂掉。因为注册的是临时节点，客户端挂掉，ZK 会进行感知，也就会把这个临时节点删除，锁也就随着释放。 业务线程假死。业务线程并没有消息，而是一个假死状态，(例如死循环，死锁，超长 GC)，这个时候锁会被一直霸占不能释放，这个问题需要从两个方面进行解决。 第一个是本身业务代码的问题，为何会出现死循环，死锁等问题;第二个是对锁的异常监控问题，这个其实也是微服务治理的一个方面。 ZK 分布式锁的 GC 问题 刚刚说了 ZK 锁的维持是靠 ZK 和客户端的心跳进行维持，如果客户端出现了长时间的 GC 会出现什么状况： ①业务线程-1 获取到锁，但未开始执行业务。 ②业务线程-2 发生长时间的 GC。 ③业务线程-1 和 ZK 的心跳发生断链。 ④lock0001 的临时节点因为心跳断链而被删除。 ⑤业务线程-2 获取到锁。 ⑥业务线程-2 开始执行业务。 ⑦业务线程-1 GC完毕，开始执行业务。 ⑧业务线程-1 和业务线程-2 同时执行业务。 基于 Etcd 分布式锁Etcd 分布式锁的实现原理Etcd 实现分布式锁比 ZK 要简单很多，就是使用 Key Value 的方式进行写入。 在集群中，如果存在 Key 的话就不能写入，也就意味着不能获取到锁，如果集群中，可以写入 Key，就意味着获取得到锁。 Etcd 到使用了 Raft 保证了集群的一致性，也就是在外界看来，只要 Etcd 集群中某一台机器存在了锁，所有的机器也就存在了锁。 这个跟 ZK 一样属于强一致性，并且数据是可以进行持久化，默认数据一更新就持久化。 锁的租期续约问题Etcd 并不存在一个心跳的机制，所以跟 Redis 一样获取锁的时候就要对其进行 Expire 的指定，这个时候就存在一个锁的租期问题。 租期问题有几种思路可以去解决，这里讨论其中一种：在获取到锁的业务线程，可以开启一个子线程去维护和轮训这把锁的有效时间，并定时的对这把锁进行续租。 假设业务线程获取到一把锁，锁的 Expire 时间为 10s，业务线程会开启一个子线程通过轮训的方式每 2 秒钟去把这把锁进行续租，每次都将锁的 Expire 还原到 10s。 当业务线程执行完业务时，会把这把锁进行删除，事件完毕。 这种思路一样会存在问题： 客户端挂掉，业务线程和续租子线程都会挂掉，锁最终会释放。 业务线程假死，这个跟 ZK 的假死情况一样，也是属于业务代码应该解决的问题。 客户端超长 GC 问题，长 GC 导致续租子进程没有进行及时续租，锁被超时释放。(GC 的问题可能是个极端问题，一般 GC 超过几秒就可能去查看问题了) 总结首先得了解清楚我们使用分布式锁的场景，为何使用分布式锁，用它来帮我们解决什么问题，先聊场景后聊分布式锁的技术选型。 无论是 Redis，ZK，Etcd，其实在各个场景下或多或少都存在一些问题，例如： Redis 的 AP 模型会限制很多使用场景，但它却拥有了几者中最高的性能。 ZK 的分布式锁要比 Redis 可靠很多，但他繁琐的实现机制导致了它的性能不如 Redis，而且 ZK 会随着集群的扩大而性能更加下降。 Etcd 看似是一种折中的方案，不过像锁的租期续约都要自己去实现。 简单来说，先了解业务场景，后进行技术选型。 为你推荐: 「如何设计」高可用的分布式锁 细谈分布式锁及在OpenStack上的应用：如何实现Active/Active高可用 自动化高可用：基于数据库的分布式锁 灵魂一问：深度强化学习终到尽头？ 相关软件推荐: 高性能分布式 RPC 框架 commonrpc 分布式对象图 NetworkObjects 分布式lua开发框架 distri.lua 分布式消息推送服务 GoPush 分布式流控系统 dimit","comments":true,"tags":[{"name":"分布式","slug":"分布式","permalink":"http://forvoyager.github.io/tags/分布式/"},{"name":"分布式锁","slug":"分布式锁","permalink":"http://forvoyager.github.io/tags/分布式锁/"}]},{"title":"4.聊一聊分布式锁的设计","date":"2019-06-26T01:34:10.000Z","path":"2019/06/26/4-聊一聊分布式锁的设计/","text":"前段时间，看到redis作者发布的一篇文章《Is Redlock safe?》，Redlock是redis作者基于redis设计的分布式锁的算法。文章起因是有一位分布式的专家写了一篇文章《How to do distributed locking》，质疑Redlock的正确性。redis作者则在《Is Redlock safe?》文章中给予回应，一来一回甚是精彩。文本就为读者一一解析两位专家的争论。 在了解两位专家的争论前，让我先从我了解的分布式锁一一道来。文章中提到的分布式锁均为排他锁。 数据库锁表我第一次接触分布式锁用的是mysql的锁表。当时我并没有分布式锁的概念。只知道当时有两台交易中心服务器处理相同的业务，每个交易中心处理订单的时候需要保证另一个无法处理。于是用mysql的一张表来控制共享资源。表结构如下： 123456789CREATE TABLE `lockedOrder` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;主码&apos;, `type` tinyint(8) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;操作类别&apos;, `order_id` varchar(64) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;锁定的order_id&apos;, `memo` varchar(1024) NOT NULL DEFAULT &apos;&apos;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &apos;保存数据时间，自动生成&apos;, PRIMARY KEY (`id`), UNIQUE KEY `uidx_order_id` (`order_id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&apos;锁定中的订单&apos;; order_id记录了订单号，type和memo用来记录下是那种类型的操作锁定的订单，memo用来记录一下操作内容。这张表能完成分布式锁的主要原因正是由于把order_id设置为了UNIQUE KEY，所以同一个订单号只能插入一次。于是对锁的竞争就交给了数据库，处理同一个订单号的交易中心把订单号插入表中，数据库保证了只有一个交易中心能插入成功，其他交易中心都会插入失败。lock和unlock的伪代码也非常简单： 123456789def lock ： exec sql: insert into lockedOrder(type,order_id,memo) values (type,order_id,memo) if result == true : return true else : return falsedef unlock ： exec sql: delete from lockedOrder where order_id=&apos;order_id&apos; 读者可以发现，这个锁从功能上有几个问题： 数据库锁实现只能是非阻塞锁，即应该为tryLock，是尝试获得锁，如果无法获得则会返回失败。要改成阻塞锁，需要反复执行insert语句直到插入成功。由于交易中心的使用场景，只要一个交易中心处理订单就行了，所以这里不需要使用阻塞锁。 这把锁没有过期时间，如果交易中心锁定了订单，但异常宕机后，这个订单就无法锁定了。这里为了让锁能够失效，需要在应用层加上定时任务，去删除过期还未解锁的订单。clear_timeout_lock的伪代码很简单，只要执行一条sql即可。 12def clear_timeout_lock : exec sql : delete from lockedOrder where update_time &amp;lt; ADDTIME(NOW(),&apos;-00:02:00&apos;) 这里设置过期时间为2分钟，也是从业务场景考虑的，如果订单处理时间可能超过2分钟的话，这个时候还需要加大。 这把锁是不能重入的，意思就是即使一个交易中心获得了锁，在它为解锁前，之后的流程如果有再去获取锁的话还会失败，这样就可能出现死锁。这个问题我们当时没有处理，如果要处理这个问题的话，需要增加字段，在insert的时候，把该交易中心的标识加进来，这样再获取锁的时候， 通过select，看下锁定的人是不是自己。lock的伪代码版本如下： 12345678910def lock ： exec sql: insert into lockedOrder(type,order_id,memo) values (type,order_id,memo) if result == true : return true else : exec sql : select id from lockedOrder where order_id=&apos;order_id&apos; and memo = &apos;TradeCenterId&apos; if count &amp;gt; 0 : return true else return false 在锁定失败后，看下锁是不是自己，如果是自己，那依然锁定成功。不过这个方法解锁又遇到了困难，第一次unlock就把锁给释放了，后面的流程都是在没锁的情况下完成，就可能出现其他交易中心也获取到这个订单锁，产生冲突。解决这个办法的方法就是给锁加计数器，记录下lock多少次。unlock的时候，只有在lock次数为0后才能删除数据库的记录。 可以看出，数据库锁能实现一个简单的避免共享资源被多个系统操作的情况。我以前在盛大的时候，发现盛大特别喜欢用数据库锁。盛大的前辈们会说，盛大基本上实现分布式锁用的都是数据库锁。在并发量不是那么恐怖的情况下，数据库锁的性能也不容易出问题，而且由于数据库的数据具有持久化的特性，一般的应用也足够应付。但是除了上面说的数据库锁的几个功能问题外，数据库锁并没有很好的应付数据库宕机的场景，如果数据库宕机，会带来的整个交易中心无法工作。当时我也没想过这个问题，我们整个交易系统，数据库是个单点，不过数据库实在是太稳定了，两年也没出过任何问题。随着工作经验的积累，构建高可用系统的概念越来越强，系统中是不允许出现单点的。现在想想，通过数据库的同步复制，以及使用vip切换Master就能解决这个问题。 缓存锁后来我开始接触缓存服务，知道很多应用都把缓存作为分布式锁，比如redis。使用缓存作为分布式锁，性能非常强劲，在一些不错的硬件上，redis可以每秒执行10w次，内网延迟不超过1ms，足够满足绝大部分应用的锁定需求。 redis锁定的原理是利用setnx命令，即只有在某个key不存在情况才能set成功该key，这样就达到了多个进程并发去set同一个key，只有一个进程能set成功。 仅有一个setnx命令，redis遇到的问题跟数据库锁一样，但是过期时间这一项，redis自带的expire功能可以不需要应用主动去删除锁。而且从 Redis 2.6.12 版本开始，redis的set命令直接直接设置NX和EX属性，NX即附带了setnx数据，key存在就无法插入，EX是过期属性，可以设置过期时间。这样一个命令就能原子的完成加锁和设置过期时间。 缓存锁优势是性能出色，劣势就是由于数据在内存中，一旦缓存服务宕机，锁数据就丢失了。像redis自带复制功能，可以对数据可靠性有一定的保证，但是由于复制也是异步完成的，因此依然可能出现master节点写入锁数据而未同步到slave节点的时候宕机，锁数据丢失问题。 分布式缓存锁—Redlockredis作者鉴于单点redis作为分布式锁的可能出现的锁数据丢失问题，提出了Redlock算法，该算法实现了比单一节点更安全、可靠的分布式锁管理（DLM）。下面我就介绍下Redlock的实现。 Redlock算法假设有N个redis节点，这些节点互相独立，一般设置为N=5，这N个节点运行在不同的机器上以保持物理层面的独立。 算法的步骤如下： 1、客户端获取当前时间，以毫秒为单位。 2、客户端尝试获取N个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N个节点以相同的key和value获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是10s，那么接口超时大概设置5-50ms。这样可以在有redis节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用。 3、客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过3个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁。 4、客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间。 5、如果客户端获取锁失败了，客户端会依次删除所有的锁。 使用Redlock算法，可以保证在挂掉最多2个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于redis的高效性能，分布式缓存锁性能并不比数据库锁差。 分布式专家质疑Redlock介绍了Redlock，就可以说起文章开头提到了分布式专家和redis作者的争论了。 该专家提到，考虑分布式锁的时候需要考虑两个方面：性能和正确性。 如果使用高性能的分布式锁，对正确性要求不高的场景下，那么使用缓存锁就足够了。 如果使用可靠性高的分布式锁，那么就需要考虑严格的可靠性问题。而Redlock则不符合正确性。为什么不符合呢？专家列举了几个方面。 现在很多编程语言使用的虚拟机都有GC功能，在Full GC的时候，程序会停下来处理GC，有些时候Full GC耗时很长，甚至程序有几分钟的卡顿，文章列举了HBase的例子，HBase有时候GC几分钟，会导致租约超时。而且Full GC什么时候到来，程序无法掌控，程序的任何时候都可能停下来处理GC，比如下图，客户端1获得了锁，正准备处理共享资源的时候，发生了Full GC直到锁过期。这样，客户端2又获得了锁，开始处理共享资源。在客户端2处理的时候，客户端1 Full GC完成，也开始处理共享资源，这样就出现了2个客户端都在处理共享资源的情况。 专家给出了解决办法，如下图，看起来就是MVCC，给锁带上token，token就是version的概念，每次操作锁完成，token都会加1，在处理共享资源的时候带上token，只有指定版本的token能够处理共享资源。 然后专家还说到了算法依赖本地时间，而且redis在处理key过期的时候，依赖gettimeofday方法获得时间，而不是monotonic clock，这也会带来时间的不准确。比如一下场景，两个客户端client 1和client 2，5个redis节点nodes (A, B, C, D and E)。 1、client 1从A、B、C成功获取锁，从D、E获取锁网络超时。 2、节点C的时钟不准确，导致锁超时。 3、client 2从C、D、E成功获取锁，从A、B获取锁网络超时。 4、这样client 1和client 2都获得了锁。 总结专家关于Redlock不可用的两点： 1、GC等场景可能随时发生，并导致在客户端获取了锁，在处理中超时，导致另外的客户端获取了锁。专家还给出了使用自增token的解决方法。 2、算法依赖本地时间，会出现时钟不准，导致2个客户端同时获得锁的情况。 所以专家给出的结论是，只有在有界的网络延迟、有界的程序中断、有界的时钟错误范围，Redlock才能正常工作，但是这三种场景的边界又是无法确认的，所以专家不建议使用Redlock。对于正确性要求高的场景，专家推荐了Zookeeper，关于使用Zookeeper作为分布式锁后面再讨论。 redis作者解疑Redlockredis作者看到这个专家的文章后，写了一篇博客予以回应。作者很客气的感谢了专家，然后表达出了对专家观点的不认同。 I asked for an analysis in the original Redlock specification here: http://redis.io/topics/distlock. So thank you Martin. However I don’t agree with the analysis. redis作者关于使用token解决锁超时问题可以概括成下面五点： 观点1，使用分布式锁一般是在，你没有其他方式去控制共享资源了，专家使用token来保证对共享资源的处理，那么就不需要分布式锁了。 观点2，对于token的生成，为保证不同客户端获得的token的可靠性，生成token的服务还是需要分布式锁保证服务的可靠性。 观点3，对于专家说的自增的token的方式，redis作者认为完全没必要，每个客户端可以生成唯一的uuid作为token，给共享资源设置为只有该uuid的客户端才能处理的状态，这样其他客户端就无法处理该共享资源，直到获得锁的客户端释放锁。 观点4、redis作者认为，对于token是有序的，并不能解决专家提出的GC问题，如上图所示，如果token 34的客户端写入过程中发送GC导致锁超时，另外的客户端可能获得token 35的锁，并再次开始写入，导致锁冲突。所以token的有序并不能跟共享资源结合起来。 观点5、redis作者认为，大部分场景下，分布式锁用来处理非事务场景下的更新问题。作者意思应该是有些场景很难结合token处理共享资源，所以得依赖锁去锁定资源并进行处理。 专家说到的另一个时钟问题，redis作者也给出了解释。客户端实际获得的锁的时间是默认的超时时间，减去获取锁所花费的时间，如果获取锁花费时间过长导致超过了锁的默认超时间，那么此时客户端并不能获取到锁，不会存在专家提出的例子。 再次分析Redlock看了两位专家你来我回的争辩，相信读者会对Redlock有了更多的认识。这里我也想就分布式专家提到的两个问题结合redis作者的观点，说说我的想法。 第一个问题我概括为，在一个客户端获取了分布式锁后，在客户端的处理过程中，可能出现锁超时释放的情况，这里说的处理中除了GC等非抗力外，程序流程未处理完也是可能发生的。之前在说到数据库锁设置的超时时间2分钟，如果出现某个任务占用某个订单锁超过2分钟，那么另一个交易中心就可以获得这把订单锁，从而两个交易中心同时处理同一个订单。正常情况，任务当然秒级处理完成，可是有时候，加入某个rpc请求设置的超时时间过长，一个任务中有多个这样的超时请求，那么，很可能就出现超过自动解锁时间了。当初我们的交易模块是用C++写的，不存在GC，如果用java写，中间还可能出现Full GC，那么锁超时解锁后，自己客户端无法感知，是件非常严重的事情。我觉得这不是锁本身的问题，上面说到的任何一个分布式锁，只要自带了超时释放的特性，都会出现这样的问题。如果使用锁的超时功能，那么客户端一定得设置获取锁超时后，采取相应的处理，而不是继续处理共享资源。Redlock的算法，在客户端获取锁后，会返回客户端能占用的锁时间，客户端必须处理该时间，让任务在超过该时间后停止下来。 第二个问题，自然就是分布式专家没有理解Redlock。Redlock有个关键的特性是，获取锁的时间是锁默认超时的总时间减去获取锁所花费的时间，这样客户端处理的时间就是一个相对时间，就跟本地时间无关了。 由此看来，Redlock的正确性是能得到很好的保证的。仔细分析Redlock，相比于一个节点的redis，Redlock提供的最主要的特性是可靠性更高，这在有些场景下是很重要的特性。但是我觉得Redlock为了实现可靠性，却花费了过大的代价。 首先必须部署5个节点才能让Redlock的可靠性更强。 然后需要请求5个节点才能获取到锁，通过Future的方式，先并发向5个节点请求，再一起获得响应结果，能缩短响应时间，不过还是比单节点redis锁要耗费更多时间。 然后由于必须获取到5个节点中的3个以上，所以可能出现获取锁冲突，即大家都获得了1-2把锁，结果谁也不能获取到锁，这个问题，redis作者借鉴了raft算法的精髓，通过冲突后在随机时间开始，可以大大降低冲突时间，但是这问题并不能很好的避免，特别是在第一次获取锁的时候，所以获取锁的时间成本增加了。 如果5个节点有2个宕机，此时锁的可用性会极大降低，首先必须等待这两个宕机节点的结果超时才能返回，另外只有3个节点，客户端必须获取到这全部3个节点的锁才能拥有锁，难度也加大了。 如果出现网络分区，那么可能出现客户端永远也无法获取锁的情况。 分析了这么多原因，我觉得Redlock的问题，最关键的一点在于Redlock需要客户端去保证写入的一致性，后端5个节点完全独立，所有的客户端都得操作这5个节点。如果5个节点有一个leader，客户端只要从leader获取锁，其他节点能同步leader的数据，这样，分区、超时、冲突等问题都不会存在。所以为了保证分布式锁的正确性，我觉得使用强一致性的分布式协调服务能更好的解决问题。 更好的分布式锁—zookeeper提到分布式协调服务，自然就想到了zookeeper。zookeeper实现了类似paxos协议，是一个拥有多个节点分布式协调服务。对zookeeper写入请求会转发到leader，leader写入完成，并同步到其他节点，直到所有节点都写入完成，才返回客户端写入成功。 zookeeper还有\u0007几个特质，让它非常适合作为分布式锁服务。 zookeeper支持watcher机制，这样实现阻塞锁，可以watch锁数据，等到数据被删除，zookeeper会通知客户端去重新竞争锁。 zookeeper的数据可以支持临时节点的概念，即客户端写入的数据是临时数据，在客户端宕机后，临时数据会被删除，这样就实现了锁的异常释放。使用这样的方式，就不需要给锁增加超时自动释放的特性了。 zookeeper实现锁的方式是客户端一起竞争写某条数据，比如/path/lock，只有第一个客户端能写入成功，其他的客户端都会写入失败。写入成功的客户端就获得了锁，写入失败的客户端，注册watch事件，等待锁的释放，从而继续竞争该锁。 如果要实现tryLock，那么竞争失败就直接返回false即可。 zookeeper实现的分布式锁简单、明了，分布式锁的关键技术都由zookeeper负责实现了。可以看下《从Paxos到Zookeeper:分布式一致性原理与实践》书里贴出来的分布式锁实现步骤 需要使用zookeeper的分布式锁功能，可以使用curator-recipes库。Curator是Netflix开源的一套ZooKeeper客户端框架，curator-recipes库里面集成了很多zookeeper的应用场景，分布式锁的功能在org.apache.curator.framework.recipes.locks包里面，《跟着实例学习ZooKeeper的用法： 分布式锁》文章里面详细的介绍了curator-recipes分布式锁的使用，想要使用分布式锁功能的朋友们不妨一试。 总结文章写到这里，基本把我关于分布式锁的了解介绍了一遍。可以实现分布式锁功能的，包括数据库、缓存、分布式协调服务等等。根据业务的场景、现状以及已经依赖的服务，应用可以使用不同分布式锁实现。文章介绍了redis作者和分布式专家关于Redlock，虽然最终觉得Redlock并不像分布式专家说的那样缺乏正确性，不过我个人觉得，如果需要最可靠的分布式锁，还是使用zookeeper会更可靠些。curator-recipes库封装的分布式锁，java应用也可以直接使用。而且如果开始依赖zookeeper，那么zookeeper不仅仅提供了分布式锁功能，选主、服务注册与发现、保存元数据信息等功能都能依赖zookeeper，这让zookeeper不会那么闲置。 参考资料： [1]《Distributed locks with Redis》 [2]《Is Redlock safe?》 [3]《How to do distributed locking》 [4]《跟着实例学习ZooKeeper的用法： 分布式锁》 [5]《从Paxos到Zookeeper:分布式一致性原理与实践》 来源：魏子珺的博客","comments":true,"tags":[{"name":"分布式","slug":"分布式","permalink":"http://forvoyager.github.io/tags/分布式/"},{"name":"分布式锁","slug":"分布式锁","permalink":"http://forvoyager.github.io/tags/分布式锁/"}]},{"title":"3.别让手机偷走孩子的注意力","date":"2019-06-17T01:12:52.000Z","path":"2019/06/17/3-别让手机偷走孩子的注意力/","text":"“妈妈，我好无聊，我要看手机。” 每当这个时候，大部分家长都经不住孩子的哼唧，也认为看个十几分钟没多大关系，于是掏出手机，递给孩子，换自己一阵安静。 尤其是在排队时、等吃饭时、写完作业后，这些无聊的碎片时间，孩子更是坐立不安。 但这些被手机侵占的碎片时间，却正在拉开孩子间的距离。 你的做法里，藏着孩子二十年后的未来。 去年春季，一篇名为《别让手机偷走你的梦想》的演讲火爆朋友圈，南京第九中学的张恒柱校长在演讲里，让所有父母和孩子警惕，不要沦为“手机控”。 在今年春季的开学典礼上，张恒柱校长再次指出一个很严肃的事实： “今天，我们的社会已始进入泛娱乐化时代，我们时刻被海量信息包围着，想安静地看会儿书，做几道题，手机来了消息，网页跳出弹窗，不是这个明星绯闻，就是“速看、震惊”这些标题党。 但等你打开，满足好奇心后，看书学习这件事，早已被冲击得支离破碎。” 是的，在这个信息爆炸的时代，只要我们一有时间，注意力都被手机占去了。 孩子长时间浸泡在手机提供的“低思考、高娱乐、低成本、高回报”的感官刺激里，有多少孩子还能够投入到枯燥的、需要不断努力才能看到成果的学习中去？ 然而，我们依旧在允许手机吞噬孩子的注意力。2016年的一个调查数据显示，家长平均每天愿意给孩子使用移动终端的累计时长约为40分钟。 泛娱乐化的时代，孩子的注意力正在被手机偷走。 注意力被偷走的孩子，正在离成功越来越远。 美国心理学家朱利安·斯坦利在1971年启动了一个超常儿童研究项目，在45年时间里跟踪了美国五千名全国排名1%的超常儿童的职业和成就。 研究结果证明，这五千名儿童绝大部分成为了一流的科学家、世界500强的CEO、联邦法官，包括扎克伯格、谢尔盖·布林等人。 研究发现，专注力和整个学习的关联性远高于智商，一个人越专注，他将来获得成就的比例就越高。 泛娱乐化时代，当大家的注意力都被碎片化阅读和爆炸性信息吞噬时，长时间的专注能力，已成为孩子最重要的竞争力之一。 如何提高注意力如何提高孩子的注意力？并不是不让孩子看手机就可以了。 “你给我专心点！” “看这里，别看其他地方！” “你又干嘛去了，快回来写作业！” 哪怕孩子被你压在书桌前，自身专注力不够的孩子，还是一样会走神。 要从根本上提升孩子的注意力，首先要科学地认知注意力。 关于人类注意力的基本规则和普遍情况，澳大利亚的心理学家莉·沃特斯在其著作《优势教养》中给出了这样的一组数据： 儿童，毫无疑问，不擅长集中和保持注意力，多数3岁的孩子只能保持专注3~5分钟。 在6~12岁之间，孩子的注意力会有一个突飞猛进的发展期，孩子的专注时间可以提高到10分钟左右。 在15岁左右，注意力集中能力又会有一个飞跃，这是大脑发展和髓鞘化增多的结果，让我们的注意力可以保持20~25分钟。 此后，我们的注意力水平趋于稳定，所以作为成年人，我们的注意力水平并没有比青少年时高很多。 据沃特斯博士介绍，成年人工作时之所以能够一直专注在电脑或工作台前，并不是我们能连续好几个小时保持专注，而是我们在不断地重新集中注意力，重新保持专注。 回想一下，当我们来到办公室后，打开电脑，专注一段时间后，也会随手喝口水，也会两眼看看远处思考另一个问题，也会突然想到早上忘记提醒孩子带水杯，只是我们又能很快地专注回来罢了。 而我们经常要求孩子把注意力集中在一件事情上，并长时间保持这种状态，不因为别的事情分心。 我们的要求太高了。 这样不是在提高孩子的注意力，而是在强迫他们坐在书桌前而已。 真正提高孩子的注意力，应该是引导孩子自发地十分专注于某一件事，让他增强这个能力。 美国心理学家朱利安· 斯坦利的研究中也指出，学生只有达到在外界看起来十分专注、刻苦、自觉的学习，而自身认为十分自然的时候，学习就进入了有效的深度学习阶段。我们经常看到，孩子在玩乐高积木时一玩就是一个小时，吃饭喝水上厕所都给忘了，这时候孩子就已经进入了深度专注的状态。培养孩子这种自发的注意力，才是关键。 注意力训练场景注意力的形成是一个体系化过程，需要创造一定的环境，以及一定的刻意练习。 但这并不代表只有专业人士才能训练孩子的注意力。只要家长掌握了一定的原理和方法，就可以从生活中找到很多机会去训练孩子的注意力。 尤其是那些被手机侵占的碎片时间，用来做注意力的训练，最合适不过。 等车、排队、学习间歇等碎片时间，跟孩子玩一些有趣的游戏，用孩子喜欢的方式去加强注意力的训练，不知不觉中，孩子的注意力，就“高人一等”了。 等车、坐车等车或坐车时，由于不方便使用道具，所以最好的注意力训练道具就是周围所能看到的一切。 让孩子找车牌号码中的某个数字，找店铺招牌上的某个文字，找这个路口的路牌在哪里，找红绿灯周围有没有摄像头……路上的一切，都可以用来提高孩子的注意力。 平时跟娇娇玩的时候，我就发现，她找出来的数量越多，越兴奋，越想玩，就越专注去找，不知不觉中，就已经进入深度专注。 这个游戏成为我和娇娇等车、坐车的主旋律之后，她的各种无聊、晕车、扭捏，都不见踪影了。 排队、等吃饭1、妈妈背包里有什么：一般出行，为了训练孩子注意力特意拿上道具，没太大必要。妈妈的背包，就是一个很好的道具。让孩子看3-5秒钟背包里有什么物品，然后拉上背包拉链，问孩子背包里头有什么。每玩一次，可以将背包里的某一样物品拿出来之后，再跟孩子玩下一轮。除了说出物品名称之外，还可以让孩子说说物品的颜色，换着法子玩。这个游戏，注意力、观察力、记忆力都能加强，一举三得。 2、接数字：玩这个游戏的孩子一定要达到能流利数数的阶段，特别适合在幼儿园刚刚学了数字的孩子。家长说出几个数字，孩子要接着往下数出一样多的数。例如，家长说1、2、3，孩子就要说4、5、6；家长说7、8，宝宝就要说9、10…这个游戏不仅能训练孩子的专注力，还可以引导孩子学会更多的数字。 3、大小西瓜：这个游戏至少需要3个人，最好拉上爸爸一起玩。第一个人当排头，说“大西瓜”，但两手比成小西瓜的样子；接着第二个人说：“小西瓜”，但两手比成大西瓜的样子，依次直到最后一人。类似的游戏还有，妈妈说“左手”，孩子举起右手；妈妈说“往前一步”，孩子就向后一步。这种类型的游戏，可以破除思维的定式和思维惯性，也是训练孩子专注力的一种方法。 学习间歇、写完作业1、抓手指游戏：妈妈可以随意从书上找一段文字，找到文字里出现频率较高的某一个字。如：弯弯的月儿小小的船。小小的船儿两头尖。我在小小的船里坐，只看见闪闪的星星蓝蓝的天。让孩子把食指放在妈妈的掌心附近，当妈妈念到文字里的“小”字时，妈妈就会抓孩子的食指，孩子听到“小”字时就要快速抽离食指躲避。每次玩这个游戏时，家里总是一片笑声。娇娇长时间学习后紧张的神经，也会因为这个游戏小小的刺激而放松很多，然后又能很好地投入到下一阶段的学习。 2、衣服拍一下、食物拍两下：这个游戏可以结合语文、英语等学习过的词汇来玩。在一年级的英语里，每一个单元学习的都是相同类型物品的词汇，妈妈就可以挑出一组衣服类的单词，和一组食物类的单词，混合在一起。当妈妈念到衣服类的单词时，孩子拍一下掌，当妈妈念到食物类的单词时，孩子拍两下掌。这样既能让孩子在学习间隙放松，又无形中帮助孩子强化所学到的单词。效果杠杠的。 3、6的倍数：这个游戏适合大一点的孩子，跟大人一起玩。一人说一个数，但是轮到带有6的数字，以及6的倍数的数字的人，就不能说话，用拍一下掌代替。这个游戏，非常锻炼孩子的注意力和大脑的运算能力，大人有时候都会算错哦。 法国生物学家乔治.居维叶说：“天才，首先是注意力。” 这个注意力，应该是孩子自发的、自然集中的注意力，而非大人强迫的。 把孩子被手机偷走的那些碎片时间，拿来跟孩子一起玩这些有趣的注意力训练游戏，孩子一定能在游戏中，不知不觉地，玩出一身专注的能力，玩出一个不一样的未来。 来源：娇娇妈（ID: jiaojiaoma8）","comments":true,"tags":[{"name":"未分类","slug":"未分类","permalink":"http://forvoyager.github.io/tags/未分类/"}]},{"title":"2.代码生成器","date":"2019-06-14T09:01:02.000Z","path":"2019/06/14/2-代码生成器/","text":"节省时间去陪老婆，就靠你了…… 代码生成工具用于在项目开发过程中生成mapper, mapper xml, dao, service, controller等基础代码，让开发人员不必花费过多的时间在这些基础代码的开发上。 目前生成的是微服务架构代码格式，如果需要MVC风格代码，忽略掉*-client-starter模块的内容即可。 先贴代码…… 使用方法代码信息配置在CodeGenerator类中按照项目/模块的需要，配置好项目信息、作者信息及数据库信息，然后添加需要生成基础代码的表。 12345678910111213141516171819202122232425// 项目名称String projectName = \"micro_service\";// 基础包名String basePackageName = \"com.xr\";// 模块名称String moduleName = \"account\";// 模块名前缀String modulePrefix = \"ms-\";// 作者String author = \"forvoyager@outlook.com\";// 代码存放路径String outputPath = \"./code\";// 数据库配置String url = \"jdbc:mysql://localhost:3306/ms_account_db?characterEncoding=UTF-8\";String driver = \"com.mysql.jdbc.Driver\";String username = \"root\";String password = \"123456\";// 需要去掉的表前缀String skipTablePrefix = \"ms_\";// 需要生成代码的表List&lt;String&gt; tables = new ArrayList&lt;String&gt;();tables.add(\"ms_account\");tables.add(\"ms_user_level\"); 注： 表及字段信息会从数据库获取，所以建表要规范，相关备注、默认值等配置好。 表名中如果有下划线，会转换为驼峰命名规则，如表：ms_funds_data生成的代码形如FundsDataModel，字段名不受此影响。 生成代码执行com.xr.code.generate.CodeGenerator类，会在指定路径（默认是在当前项目路径下）生成代码文件。 最终生成的代码样例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657micro_service └─ms-account ├─ms-account-client-starter │ └─src │ └─main │ └─java │ └─com │ └─xr │ └─account │ └─client │ AccountClient.java │ UserLevelClient.java │ ├─ms-account-common │ └─src │ └─main │ └─java │ └─com │ └─xr │ └─account │ └─common │ ├─controller │ │ IAccountController.java │ │ IUserLevelController.java │ │ │ └─model │ AccountModel.java │ UserLevelModel.java │ └─ms-account-service └─src └─main ├─java │ └─com │ └─xr │ └─account │ ├─controller │ │ AccountController.java │ │ UserLevelController.java │ │ │ ├─mapper │ │ AccountMapper.java │ │ UserLevelMapper.java │ │ │ └─service │ │ IAccountService.java │ │ IUserLevelService.java │ │ │ └─impl │ AccountServiceImpl.java │ UserLevelServiceImpl.java │ └─resources └─mybatis └─mapper account.xml userLevel.xml 其他一些约定 默认所有表都有如下三个字段 123create_time 创建时间update_time 最后修改时间version 数据版本号（用于乐观锁实现） 所有表的更新操作，版本号自动++ 1UPDATE `table` SET version = version + 1 项目中如下路径的文件是代码模板文件，按需调整。 1code-generator/src/main/resources/tpl 项目中如下路径的文件是mybatis基础配置文件，按需调整。 1code-generator/src/main/resources/mybatis 项目中如下路径的文件是mapper和service的基础文件，按需调整。 1code-generator/src/main/java/com/xr/base/core/service","comments":true,"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://forvoyager.github.io/tags/JAVA/"}]},{"title":"1.github+hexo构建个人博客","date":"2019-06-13T06:12:45.000Z","path":"2019/06/13/1-github-hexo构建个人博客/","text":"第一篇就写你吧…… 安装Node.js具体安装过程此处不赘述，见官方文档，安装完成后执行下面命令验证是否安装成功： 12&gt;node -vv8.7.0 安装Git这个过程就不赘述了，见官方文档，安装完成后执行下面命令验证是否安装成功： 12&gt;git --versiongit version 2.12.2.windows.2 安装Hexo安装按顺序执行下面命令，进行安装： 12345npm install hexo-cli -ghexo init blogcd blognpm installhexo server 注：blog是文件夹名字，根据需要自行修改。 按照上面步骤操作，没有报错，正常安装完成后，访问下面地址，能打开说明安装成功。http://localhost:4000/ blog目录中的文件及目录结构（只列了第一级目录）如下所示： 1234567891011E:\\blog│├─node_modules├─public├─scaffolds├─source├─themes├─config.yml├─db.json├─package.json└─package-lock.json 主题设置默认的主题有点丑，换一个主题吧，以hexo-theme-yilia主题为例，更多主题…… 下载主题操作步骤如下： 123cd blog/themeshexo cleangit clone https://github.com/litten/hexo-theme-yilia.git yilia 修改配置修改blog目录下的_config.yml配置文件。 将theme属性，设置为yilia。 在文件最后添加如下配置，显示文章目录。123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 生成目录： 1npm i hexo-generator-json-content --save 验证主题启动本地web服务器： 1hexo server 访问http://localhost:4000/，查看新主题效果。 写文章文章是markdown文档 创建文章执行命令：1hexo new &quot;文章标题&quot; 然后会在source/_posts路径下生成markdown文件。写文章就是按markdown语法编辑此文件。 如果文章太长，可以在文章中任意你想截断的位置加上如下描述： 1&lt;!-- more --&gt; 文章会在此处截断，文章列表页显示截断之前的内容，点击“查看全文”后显示全部内容。 发布博客创建github账户并配置 注册账号，传送门…… 创建仓库 Repository name必须是这种形式：{username}.github.io 配置hexo 修改blog目录下的_config.yml配置文件中的deploy项为如下形式： 12345deploy: type: git repo: &#123;上面创建的仓库地址，如：https://github.com/username/username.github.io.git&#125; branch: master message: &quot;git commit时的备注信息，自行调整&quot; 发布执行下面命令，发布博客，其实就是生成博客页面，然后上传到github仓库中。 1hexo clean &amp;&amp; hexo deploy 提交过程中会提示输入github用户名和密码。 访问博客传送门","comments":true,"tags":[{"name":"未分类","slug":"未分类","permalink":"http://forvoyager.github.io/tags/未分类/"}]}]