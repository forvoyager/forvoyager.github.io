[{"title":"6、Understanding LSTM Networks","date":"2019-07-03T01:16:55.000Z","path":"2019/07/03/6、Understanding-LSTM-Networks/","text":"Recurrent Neural NetworksHumans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence. Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones. Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist. Recurrent Neural Networks have loops. In the above diagram, a chunk of neural network, AAA, looks at some input xtxtx_t and outputs a value hthth_t. A loop allows information to be passed from one step of the network to the next. These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop: An unrolled recurrent neural network. This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data. And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning… The list goes on. I’ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy’s excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks. But they really are pretty amazing. Essential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore. The Problem of Long-Term DependenciesOne of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends. Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information. But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large. Unfortunately, as that gap grows, RNNs become unable to learn to connect the information. In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. Thankfully, LSTMs don’t have this problem! LSTM NetworksLong Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. The repeating module in a standard RNN contains a single layer. LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way. The repeating module in an LSTM contains four interacting layers. Don’t worry about the details of what’s going on. We’ll walk through the LSTM diagram step by step later. For now, let’s just try to get comfortable with the notation we’ll be using. In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations. &lt;!– To be a bit more explicit, we can split up each line into lines carrying individual scalar values: The Core Idea Behind LSTMsThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged. The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation. The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!” An LSTM has three of these gates, to protect and control the cell state. Step-by-Step LSTM Walk ThroughThe first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1ht−1h_{t-1} and xtxtx_t, and outputs a number between 000 and 111 for each number in the cell state Ct−1Ct−1C_{t-1}. A 111 represents “completely keep this” while a 000 represents “completely get rid of this.” Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject. The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, CtCt\\tilde{C}_t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state. In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting. It’s now time to update the old cell state, Ct−1Ct−1C_{t-1}, into the new cell state CtCtC_t. The previous steps already decided what to do, we just need to actually do it. We multiply the old state by ftftf_t, forgetting the things we decided to forget earlier. Then we add it∗Ctit∗Cti_t*\\tilde{C}_t. This is the new candidate values, scaled by how much we decided to update each state value. In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps. Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanhtanh\\tanh (to push the values to be between −1−1-1 and 111) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to. For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next. Variants on Long Short Term MemoryWhat I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them. One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state. The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others. Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015). There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014). Which of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that they’re all about the same. Jozefowicz, et al. (2015) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks. ConclusionEarlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks! Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable. LSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, Xu, et al. (2015) do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner… Attention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by Kalchbrenner, et al. (2015) seem extremely promising. Work using RNNs in generative models – such as Gregor, et al. (2015), Chung, et al. (2015), or Bayer &amp; Osendorfer (2015) – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so! AcknowledgmentsI’m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post. I’m very grateful to my colleagues at Google for their helpful feedback, especially Oriol Vinyals, Greg Corrado, Jon Shlens, Luke Vilnis, and Ilya Sutskever. I’m also thankful to many other friends and colleagues for taking the time to help me, including Dario Amodei, and Jacob Steinhardt. I’m especially thankful to Kyunghyun Cho for extremely thoughtful correspondence about my diagrams. Before this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback. In addition to the original authors, a lot of people contributed to the modern LSTM. A non-comprehensive list is: Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Graves.↩ More Posts Attention and Augmented Recurrent Neural Networks A Modular Perspective Neural Networks, Manifolds, and Topology Deep Learning, NLP, and Representations 来源： colah’s blogGithub","comments":true,"tags":[{"name":"算法","slug":"算法","permalink":"http://forvoyager.github.io/tags/算法/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forvoyager.github.io/tags/机器学习/"}]},{"title":"5、如何设计高可用的分布式锁","date":"2019-06-27T03:32:00.000Z","path":"2019/06/27/5、如何设计高可用的分布式锁/","text":"分布式锁在分布式环境下，锁定全局唯一公共资源，表现为：请求串行化互斥性 第一步是上锁的资源目标，是锁定全局唯一公共资源，只有是全局唯一的资源才存在多个线程或服务竞争的情况。 互斥性表现为一个资源的隔离级别串行化，如果对照单机事务 ACID 的隔离性来说，互斥性的事务隔离级别是 SERLALIZABLE，属于最高的隔离级别。 事务隔离级别：DEFAULTREAD_UNCOMMITTEDREAD_COMMITEDREPEATABLE_READSERLALIZABLE 分布式锁目的分布式锁的目的如下： 解决业务层幂等性 解决 MQ 消费端多次接受同一消息 确保串行|隔离级别 多台机器同时执行定时任务 寻找唯一资源进行上锁例子： 防止用户重复下单 共享资源进行上锁的对象 ： 【用户id】 订单生成后发送MQ给消费者进行积分的添加 寻找上锁的对象 ：【订单id】 用户已经创建订单，准备对订单进行支付，同时商家在对这个订单进行改价 寻找上锁对象 ： 【订单id】 基于 Redis 分布式锁Redis 单线程串行处理天然就是解决串行化问题，用来解决分布式锁是再适合不过。 实现方式： 12setnx key value Expire_time 获取到锁 返回 1 ， 获取失败 返回 0 存在问题如下 锁时间不可控 Redis 只能在 Setnx 指定一个锁的超时时间，假设初始设定锁的时间是 10 秒钟，但是业务获取到锁跑了 20 秒钟，在 10 秒钟之后，如果又有一个业务可以获取到相同的一把锁。 这个时候可能就存在两个相同的业务都获取得到锁的问题，并且两个业务处在并行阶段。也就是第一个获取锁的业务无法对自身的锁进行续租。 单点连接超时问题 Redis 的 Client 与 Server 端并没有维持心跳的机制，如果在连接中出现问题，Client 会得到一个超时的回馈。 主从问题 Redis 的集群实际上在 CAP 模式中是处在与 AP 的模型，保证可用性。在主从复制中“主”有数据，但可能“从”还没有数据。这个时候，一旦主挂掉或者网络抖动等各种原因，可能会切换到“从”节点。 这个时候有可能会导致两个业务线程同时的获取到两把锁： ①业务线程-1：向主节点请求锁 ②业务线程-1：获取锁 ③业务线程-1：获取到锁并开始执行业务 ④这个时候 Redis 刚生成的锁在主从之间还未进行同步 ⑤Redis 这时候主节点挂掉了 ⑥Redis 的从节点升级为主节点 ⑦业务线程-2：向新的主节点请求锁 ⑧业务线程-2：获取到新的主节点返回的锁 ⑨业务线程-2：获取到锁开始执行业务 ⑩这个时候业务线程-1和业务线程-2同时在执行任务 Redlock 上述的问题其实并不是 Redis 的缺陷，只是 Redis 采用了 AP 模型，它本身无法确保我们对一致性的要求。 Redis 官方推荐 Redlock 算法来保证，问题是 Redlock 至少需要三个 Redis 主从实例来实现，维护成本比较高。 相当于 Redlock 使用三个 Redis 集群实现了自己的另一套一致性算法，比较繁琐，在业界也使用得比较少。 能不能使用 Redis 作为分布式锁能不能使用 Redis 作为分布式锁，这个本身就不是 Redis 的问题，还是取决于业务场景，我们先要自己确认我们的场景是适合 AP 还是 CP。 如果在社交发帖等场景下，我们并没有非常强的事务一致性问题，Redis 提供给我们高性能的 AP 模型是非常适合的。 但如果是交易类型，对数据一致性非常敏感的场景，我们可能要寻找一种更加适合的 CP 模型。 Redis 可能作为高可用的分布式锁并不合适，我们需要确立高可用分布式锁的设计目标。 高可用分布式锁设计目标高可用分布式锁的设计目标如下：强一致性，是 CP 模型服务高可用，不存在单点问题锁能够续租和自动释放业务接入简单 三种分布式锁方案对比常用的三种分布式锁方案对比如下图： 基于 Zookeeper 分布式锁刚刚也分析过，Redis 其实无法确保数据的一致性，先来看 Zookeeper 是否合适作为我们需要的分布式锁。 首先 ZK 的模式是 CP 模型，也就是说，当 ZK 锁提供给我们进行访问的时候，在 ZK 集群中能确保这把锁在 ZK 的每一个节点都存在。 这个实际上是 ZK 的 Leader 通过二阶段提交写请求来保证的，这个也是 ZK 的集群规模大了的一个瓶颈点。 ZK 锁实现的原理说 ZK 的锁问题之前先看看 Zookeeper 中的几个特性，这几个特性构建了 ZK 的一把分布式锁。 Zookeeper 中的几个特性如下： 有序节点，当在一个父目录下如 /lock 下创建有序节点，节点会按照严格的先后顺序创建出自节点 lock000001，lock000002，lock0000003，以此类推，有序节点能严格保证各个自节点按照排序命名生成。 临时节点，客户端建立了一个临时节点，在客户端的会话结束或会话超时，Zookeeper 会自动删除该节点 ID。 事件监听，在读取数据时，我们可以对节点设置监听，当节点的数据发生变化(1 节点创建，2 节点删除，3 节点数据变动，4 子节点变动)时，Zookeeper 会通知客户端。 结合这几个特点，来看下 ZK 是怎么组合分布式锁： 业务线程-1，业务线程-2 分别向 ZK 的 /lock 目录下，申请创建有序的临时节点。 业务线程-1 抢到 /lock0001 的文件，也就是在整个目录下最小序的节点，也就是线程-1 获取到了锁。 业务线程-2 只能抢到 /lock0002 的文件，并不是最小序的节点，线程 2 未能获取锁。 业务线程-1 与 lock0001 建立了连接，并维持了心跳，维持的心跳也就是这把锁的租期。 当业务线程-1 完成了业务，将释放掉与 ZK 的连接，也就是释放了这把锁。 ZK 分布式锁的代码实现ZK 官方提供的客户端并不支持分布式锁的直接实现，我们需要自己写代码去利用 ZK 的这几个特性去进行实现： ZK 分布式锁客户端假死的问题客户端创建了临时有序节点并建立了事件监听，就可以让业务线程与 ZK 维持心跳，这个心跳也就是这把锁的租期。 当客户端的业务线程完成了执行就把节点进行删除，也就释放了这把锁，不过中间也可能存在问题： 客户端挂掉。因为注册的是临时节点，客户端挂掉，ZK 会进行感知，也就会把这个临时节点删除，锁也就随着释放。 业务线程假死。业务线程并没有消息，而是一个假死状态，(例如死循环，死锁，超长 GC)，这个时候锁会被一直霸占不能释放，这个问题需要从两个方面进行解决。 第一个是本身业务代码的问题，为何会出现死循环，死锁等问题;第二个是对锁的异常监控问题，这个其实也是微服务治理的一个方面。 ZK 分布式锁的 GC 问题 刚刚说了 ZK 锁的维持是靠 ZK 和客户端的心跳进行维持，如果客户端出现了长时间的 GC 会出现什么状况： ①业务线程-1 获取到锁，但未开始执行业务。 ②业务线程-2 发生长时间的 GC。 ③业务线程-1 和 ZK 的心跳发生断链。 ④lock0001 的临时节点因为心跳断链而被删除。 ⑤业务线程-2 获取到锁。 ⑥业务线程-2 开始执行业务。 ⑦业务线程-1 GC完毕，开始执行业务。 ⑧业务线程-1 和业务线程-2 同时执行业务。 基于 Etcd 分布式锁Etcd 分布式锁的实现原理Etcd 实现分布式锁比 ZK 要简单很多，就是使用 Key Value 的方式进行写入。 在集群中，如果存在 Key 的话就不能写入，也就意味着不能获取到锁，如果集群中，可以写入 Key，就意味着获取得到锁。 Etcd 到使用了 Raft 保证了集群的一致性，也就是在外界看来，只要 Etcd 集群中某一台机器存在了锁，所有的机器也就存在了锁。 这个跟 ZK 一样属于强一致性，并且数据是可以进行持久化，默认数据一更新就持久化。 锁的租期续约问题Etcd 并不存在一个心跳的机制，所以跟 Redis 一样获取锁的时候就要对其进行 Expire 的指定，这个时候就存在一个锁的租期问题。 租期问题有几种思路可以去解决，这里讨论其中一种：在获取到锁的业务线程，可以开启一个子线程去维护和轮训这把锁的有效时间，并定时的对这把锁进行续租。 假设业务线程获取到一把锁，锁的 Expire 时间为 10s，业务线程会开启一个子线程通过轮训的方式每 2 秒钟去把这把锁进行续租，每次都将锁的 Expire 还原到 10s。 当业务线程执行完业务时，会把这把锁进行删除，事件完毕。 这种思路一样会存在问题： 客户端挂掉，业务线程和续租子线程都会挂掉，锁最终会释放。 业务线程假死，这个跟 ZK 的假死情况一样，也是属于业务代码应该解决的问题。 客户端超长 GC 问题，长 GC 导致续租子进程没有进行及时续租，锁被超时释放。(GC 的问题可能是个极端问题，一般 GC 超过几秒就可能去查看问题了) 总结首先得了解清楚我们使用分布式锁的场景，为何使用分布式锁，用它来帮我们解决什么问题，先聊场景后聊分布式锁的技术选型。 无论是 Redis，ZK，Etcd，其实在各个场景下或多或少都存在一些问题，例如： Redis 的 AP 模型会限制很多使用场景，但它却拥有了几者中最高的性能。 ZK 的分布式锁要比 Redis 可靠很多，但他繁琐的实现机制导致了它的性能不如 Redis，而且 ZK 会随着集群的扩大而性能更加下降。 Etcd 看似是一种折中的方案，不过像锁的租期续约都要自己去实现。 简单来说，先了解业务场景，后进行技术选型。 为你推荐: 「如何设计」高可用的分布式锁 细谈分布式锁及在OpenStack上的应用：如何实现Active/Active高可用 自动化高可用：基于数据库的分布式锁 灵魂一问：深度强化学习终到尽头？ 相关软件推荐: 高性能分布式 RPC 框架 commonrpc 分布式对象图 NetworkObjects 分布式lua开发框架 distri.lua 分布式消息推送服务 GoPush 分布式流控系统 dimit","comments":true,"tags":[{"name":"分布式","slug":"分布式","permalink":"http://forvoyager.github.io/tags/分布式/"},{"name":"分布式锁","slug":"分布式锁","permalink":"http://forvoyager.github.io/tags/分布式锁/"}]},{"title":"4、聊一聊分布式锁的设计","date":"2019-06-26T01:34:10.000Z","path":"2019/06/26/4、聊一聊分布式锁的设计/","text":"前段时间，看到redis作者发布的一篇文章《Is Redlock safe?》，Redlock是redis作者基于redis设计的分布式锁的算法。文章起因是有一位分布式的专家写了一篇文章《How to do distributed locking》，质疑Redlock的正确性。redis作者则在《Is Redlock safe?》文章中给予回应，一来一回甚是精彩。文本就为读者一一解析两位专家的争论。 在了解两位专家的争论前，让我先从我了解的分布式锁一一道来。文章中提到的分布式锁均为排他锁。 数据库锁表我第一次接触分布式锁用的是mysql的锁表。当时我并没有分布式锁的概念。只知道当时有两台交易中心服务器处理相同的业务，每个交易中心处理订单的时候需要保证另一个无法处理。于是用mysql的一张表来控制共享资源。表结构如下： 123456789CREATE TABLE `lockedOrder` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;主码&apos;, `type` tinyint(8) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;操作类别&apos;, `order_id` varchar(64) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;锁定的order_id&apos;, `memo` varchar(1024) NOT NULL DEFAULT &apos;&apos;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &apos;保存数据时间，自动生成&apos;, PRIMARY KEY (`id`), UNIQUE KEY `uidx_order_id` (`order_id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&apos;锁定中的订单&apos;; order_id记录了订单号，type和memo用来记录下是那种类型的操作锁定的订单，memo用来记录一下操作内容。这张表能完成分布式锁的主要原因正是由于把order_id设置为了UNIQUE KEY，所以同一个订单号只能插入一次。于是对锁的竞争就交给了数据库，处理同一个订单号的交易中心把订单号插入表中，数据库保证了只有一个交易中心能插入成功，其他交易中心都会插入失败。lock和unlock的伪代码也非常简单： 123456789def lock ： exec sql: insert into lockedOrder(type,order_id,memo) values (type,order_id,memo) if result == true : return true else : return falsedef unlock ： exec sql: delete from lockedOrder where order_id=&apos;order_id&apos; 读者可以发现，这个锁从功能上有几个问题： 数据库锁实现只能是非阻塞锁，即应该为tryLock，是尝试获得锁，如果无法获得则会返回失败。要改成阻塞锁，需要反复执行insert语句直到插入成功。由于交易中心的使用场景，只要一个交易中心处理订单就行了，所以这里不需要使用阻塞锁。 这把锁没有过期时间，如果交易中心锁定了订单，但异常宕机后，这个订单就无法锁定了。这里为了让锁能够失效，需要在应用层加上定时任务，去删除过期还未解锁的订单。clear_timeout_lock的伪代码很简单，只要执行一条sql即可。 12def clear_timeout_lock : exec sql : delete from lockedOrder where update_time &amp;lt; ADDTIME(NOW(),&apos;-00:02:00&apos;) 这里设置过期时间为2分钟，也是从业务场景考虑的，如果订单处理时间可能超过2分钟的话，这个时候还需要加大。 这把锁是不能重入的，意思就是即使一个交易中心获得了锁，在它为解锁前，之后的流程如果有再去获取锁的话还会失败，这样就可能出现死锁。这个问题我们当时没有处理，如果要处理这个问题的话，需要增加字段，在insert的时候，把该交易中心的标识加进来，这样再获取锁的时候， 通过select，看下锁定的人是不是自己。lock的伪代码版本如下： 12345678910def lock ： exec sql: insert into lockedOrder(type,order_id,memo) values (type,order_id,memo) if result == true : return true else : exec sql : select id from lockedOrder where order_id=&apos;order_id&apos; and memo = &apos;TradeCenterId&apos; if count &amp;gt; 0 : return true else return false 在锁定失败后，看下锁是不是自己，如果是自己，那依然锁定成功。不过这个方法解锁又遇到了困难，第一次unlock就把锁给释放了，后面的流程都是在没锁的情况下完成，就可能出现其他交易中心也获取到这个订单锁，产生冲突。解决这个办法的方法就是给锁加计数器，记录下lock多少次。unlock的时候，只有在lock次数为0后才能删除数据库的记录。 可以看出，数据库锁能实现一个简单的避免共享资源被多个系统操作的情况。我以前在盛大的时候，发现盛大特别喜欢用数据库锁。盛大的前辈们会说，盛大基本上实现分布式锁用的都是数据库锁。在并发量不是那么恐怖的情况下，数据库锁的性能也不容易出问题，而且由于数据库的数据具有持久化的特性，一般的应用也足够应付。但是除了上面说的数据库锁的几个功能问题外，数据库锁并没有很好的应付数据库宕机的场景，如果数据库宕机，会带来的整个交易中心无法工作。当时我也没想过这个问题，我们整个交易系统，数据库是个单点，不过数据库实在是太稳定了，两年也没出过任何问题。随着工作经验的积累，构建高可用系统的概念越来越强，系统中是不允许出现单点的。现在想想，通过数据库的同步复制，以及使用vip切换Master就能解决这个问题。 缓存锁后来我开始接触缓存服务，知道很多应用都把缓存作为分布式锁，比如redis。使用缓存作为分布式锁，性能非常强劲，在一些不错的硬件上，redis可以每秒执行10w次，内网延迟不超过1ms，足够满足绝大部分应用的锁定需求。 redis锁定的原理是利用setnx命令，即只有在某个key不存在情况才能set成功该key，这样就达到了多个进程并发去set同一个key，只有一个进程能set成功。 仅有一个setnx命令，redis遇到的问题跟数据库锁一样，但是过期时间这一项，redis自带的expire功能可以不需要应用主动去删除锁。而且从 Redis 2.6.12 版本开始，redis的set命令直接直接设置NX和EX属性，NX即附带了setnx数据，key存在就无法插入，EX是过期属性，可以设置过期时间。这样一个命令就能原子的完成加锁和设置过期时间。 缓存锁优势是性能出色，劣势就是由于数据在内存中，一旦缓存服务宕机，锁数据就丢失了。像redis自带复制功能，可以对数据可靠性有一定的保证，但是由于复制也是异步完成的，因此依然可能出现master节点写入锁数据而未同步到slave节点的时候宕机，锁数据丢失问题。 分布式缓存锁—Redlockredis作者鉴于单点redis作为分布式锁的可能出现的锁数据丢失问题，提出了Redlock算法，该算法实现了比单一节点更安全、可靠的分布式锁管理（DLM）。下面我就介绍下Redlock的实现。 Redlock算法假设有N个redis节点，这些节点互相独立，一般设置为N=5，这N个节点运行在不同的机器上以保持物理层面的独立。 算法的步骤如下： 1、客户端获取当前时间，以毫秒为单位。 2、客户端尝试获取N个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N个节点以相同的key和value获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是10s，那么接口超时大概设置5-50ms。这样可以在有redis节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用。 3、客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过3个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁。 4、客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间。 5、如果客户端获取锁失败了，客户端会依次删除所有的锁。 使用Redlock算法，可以保证在挂掉最多2个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于redis的高效性能，分布式缓存锁性能并不比数据库锁差。 分布式专家质疑Redlock介绍了Redlock，就可以说起文章开头提到了分布式专家和redis作者的争论了。 该专家提到，考虑分布式锁的时候需要考虑两个方面：性能和正确性。 如果使用高性能的分布式锁，对正确性要求不高的场景下，那么使用缓存锁就足够了。 如果使用可靠性高的分布式锁，那么就需要考虑严格的可靠性问题。而Redlock则不符合正确性。为什么不符合呢？专家列举了几个方面。 现在很多编程语言使用的虚拟机都有GC功能，在Full GC的时候，程序会停下来处理GC，有些时候Full GC耗时很长，甚至程序有几分钟的卡顿，文章列举了HBase的例子，HBase有时候GC几分钟，会导致租约超时。而且Full GC什么时候到来，程序无法掌控，程序的任何时候都可能停下来处理GC，比如下图，客户端1获得了锁，正准备处理共享资源的时候，发生了Full GC直到锁过期。这样，客户端2又获得了锁，开始处理共享资源。在客户端2处理的时候，客户端1 Full GC完成，也开始处理共享资源，这样就出现了2个客户端都在处理共享资源的情况。 专家给出了解决办法，如下图，看起来就是MVCC，给锁带上token，token就是version的概念，每次操作锁完成，token都会加1，在处理共享资源的时候带上token，只有指定版本的token能够处理共享资源。 然后专家还说到了算法依赖本地时间，而且redis在处理key过期的时候，依赖gettimeofday方法获得时间，而不是monotonic clock，这也会带来时间的不准确。比如一下场景，两个客户端client 1和client 2，5个redis节点nodes (A, B, C, D and E)。 1、client 1从A、B、C成功获取锁，从D、E获取锁网络超时。 2、节点C的时钟不准确，导致锁超时。 3、client 2从C、D、E成功获取锁，从A、B获取锁网络超时。 4、这样client 1和client 2都获得了锁。 总结专家关于Redlock不可用的两点： 1、GC等场景可能随时发生，并导致在客户端获取了锁，在处理中超时，导致另外的客户端获取了锁。专家还给出了使用自增token的解决方法。 2、算法依赖本地时间，会出现时钟不准，导致2个客户端同时获得锁的情况。 所以专家给出的结论是，只有在有界的网络延迟、有界的程序中断、有界的时钟错误范围，Redlock才能正常工作，但是这三种场景的边界又是无法确认的，所以专家不建议使用Redlock。对于正确性要求高的场景，专家推荐了Zookeeper，关于使用Zookeeper作为分布式锁后面再讨论。 redis作者解疑Redlockredis作者看到这个专家的文章后，写了一篇博客予以回应。作者很客气的感谢了专家，然后表达出了对专家观点的不认同。 I asked for an analysis in the original Redlock specification here: http://redis.io/topics/distlock. So thank you Martin. However I don’t agree with the analysis. redis作者关于使用token解决锁超时问题可以概括成下面五点： 观点1，使用分布式锁一般是在，你没有其他方式去控制共享资源了，专家使用token来保证对共享资源的处理，那么就不需要分布式锁了。 观点2，对于token的生成，为保证不同客户端获得的token的可靠性，生成token的服务还是需要分布式锁保证服务的可靠性。 观点3，对于专家说的自增的token的方式，redis作者认为完全没必要，每个客户端可以生成唯一的uuid作为token，给共享资源设置为只有该uuid的客户端才能处理的状态，这样其他客户端就无法处理该共享资源，直到获得锁的客户端释放锁。 观点4、redis作者认为，对于token是有序的，并不能解决专家提出的GC问题，如上图所示，如果token 34的客户端写入过程中发送GC导致锁超时，另外的客户端可能获得token 35的锁，并再次开始写入，导致锁冲突。所以token的有序并不能跟共享资源结合起来。 观点5、redis作者认为，大部分场景下，分布式锁用来处理非事务场景下的更新问题。作者意思应该是有些场景很难结合token处理共享资源，所以得依赖锁去锁定资源并进行处理。 专家说到的另一个时钟问题，redis作者也给出了解释。客户端实际获得的锁的时间是默认的超时时间，减去获取锁所花费的时间，如果获取锁花费时间过长导致超过了锁的默认超时间，那么此时客户端并不能获取到锁，不会存在专家提出的例子。 再次分析Redlock看了两位专家你来我回的争辩，相信读者会对Redlock有了更多的认识。这里我也想就分布式专家提到的两个问题结合redis作者的观点，说说我的想法。 第一个问题我概括为，在一个客户端获取了分布式锁后，在客户端的处理过程中，可能出现锁超时释放的情况，这里说的处理中除了GC等非抗力外，程序流程未处理完也是可能发生的。之前在说到数据库锁设置的超时时间2分钟，如果出现某个任务占用某个订单锁超过2分钟，那么另一个交易中心就可以获得这把订单锁，从而两个交易中心同时处理同一个订单。正常情况，任务当然秒级处理完成，可是有时候，加入某个rpc请求设置的超时时间过长，一个任务中有多个这样的超时请求，那么，很可能就出现超过自动解锁时间了。当初我们的交易模块是用C++写的，不存在GC，如果用java写，中间还可能出现Full GC，那么锁超时解锁后，自己客户端无法感知，是件非常严重的事情。我觉得这不是锁本身的问题，上面说到的任何一个分布式锁，只要自带了超时释放的特性，都会出现这样的问题。如果使用锁的超时功能，那么客户端一定得设置获取锁超时后，采取相应的处理，而不是继续处理共享资源。Redlock的算法，在客户端获取锁后，会返回客户端能占用的锁时间，客户端必须处理该时间，让任务在超过该时间后停止下来。 第二个问题，自然就是分布式专家没有理解Redlock。Redlock有个关键的特性是，获取锁的时间是锁默认超时的总时间减去获取锁所花费的时间，这样客户端处理的时间就是一个相对时间，就跟本地时间无关了。 由此看来，Redlock的正确性是能得到很好的保证的。仔细分析Redlock，相比于一个节点的redis，Redlock提供的最主要的特性是可靠性更高，这在有些场景下是很重要的特性。但是我觉得Redlock为了实现可靠性，却花费了过大的代价。 首先必须部署5个节点才能让Redlock的可靠性更强。 然后需要请求5个节点才能获取到锁，通过Future的方式，先并发向5个节点请求，再一起获得响应结果，能缩短响应时间，不过还是比单节点redis锁要耗费更多时间。 然后由于必须获取到5个节点中的3个以上，所以可能出现获取锁冲突，即大家都获得了1-2把锁，结果谁也不能获取到锁，这个问题，redis作者借鉴了raft算法的精髓，通过冲突后在随机时间开始，可以大大降低冲突时间，但是这问题并不能很好的避免，特别是在第一次获取锁的时候，所以获取锁的时间成本增加了。 如果5个节点有2个宕机，此时锁的可用性会极大降低，首先必须等待这两个宕机节点的结果超时才能返回，另外只有3个节点，客户端必须获取到这全部3个节点的锁才能拥有锁，难度也加大了。 如果出现网络分区，那么可能出现客户端永远也无法获取锁的情况。 分析了这么多原因，我觉得Redlock的问题，最关键的一点在于Redlock需要客户端去保证写入的一致性，后端5个节点完全独立，所有的客户端都得操作这5个节点。如果5个节点有一个leader，客户端只要从leader获取锁，其他节点能同步leader的数据，这样，分区、超时、冲突等问题都不会存在。所以为了保证分布式锁的正确性，我觉得使用强一致性的分布式协调服务能更好的解决问题。 更好的分布式锁—zookeeper提到分布式协调服务，自然就想到了zookeeper。zookeeper实现了类似paxos协议，是一个拥有多个节点分布式协调服务。对zookeeper写入请求会转发到leader，leader写入完成，并同步到其他节点，直到所有节点都写入完成，才返回客户端写入成功。 zookeeper还有\u0007几个特质，让它非常适合作为分布式锁服务。 zookeeper支持watcher机制，这样实现阻塞锁，可以watch锁数据，等到数据被删除，zookeeper会通知客户端去重新竞争锁。 zookeeper的数据可以支持临时节点的概念，即客户端写入的数据是临时数据，在客户端宕机后，临时数据会被删除，这样就实现了锁的异常释放。使用这样的方式，就不需要给锁增加超时自动释放的特性了。 zookeeper实现锁的方式是客户端一起竞争写某条数据，比如/path/lock，只有第一个客户端能写入成功，其他的客户端都会写入失败。写入成功的客户端就获得了锁，写入失败的客户端，注册watch事件，等待锁的释放，从而继续竞争该锁。 如果要实现tryLock，那么竞争失败就直接返回false即可。 zookeeper实现的分布式锁简单、明了，分布式锁的关键技术都由zookeeper负责实现了。可以看下《从Paxos到Zookeeper:分布式一致性原理与实践》书里贴出来的分布式锁实现步骤 需要使用zookeeper的分布式锁功能，可以使用curator-recipes库。Curator是Netflix开源的一套ZooKeeper客户端框架，curator-recipes库里面集成了很多zookeeper的应用场景，分布式锁的功能在org.apache.curator.framework.recipes.locks包里面，《跟着实例学习ZooKeeper的用法： 分布式锁》文章里面详细的介绍了curator-recipes分布式锁的使用，想要使用分布式锁功能的朋友们不妨一试。 总结文章写到这里，基本把我关于分布式锁的了解介绍了一遍。可以实现分布式锁功能的，包括数据库、缓存、分布式协调服务等等。根据业务的场景、现状以及已经依赖的服务，应用可以使用不同分布式锁实现。文章介绍了redis作者和分布式专家关于Redlock，虽然最终觉得Redlock并不像分布式专家说的那样缺乏正确性，不过我个人觉得，如果需要最可靠的分布式锁，还是使用zookeeper会更可靠些。curator-recipes库封装的分布式锁，java应用也可以直接使用。而且如果开始依赖zookeeper，那么zookeeper不仅仅提供了分布式锁功能，选主、服务注册与发现、保存元数据信息等功能都能依赖zookeeper，这让zookeeper不会那么闲置。 参考资料： [1]《Distributed locks with Redis》 [2]《Is Redlock safe?》 [3]《How to do distributed locking》 [4]《跟着实例学习ZooKeeper的用法： 分布式锁》 [5]《从Paxos到Zookeeper:分布式一致性原理与实践》 来源：魏子珺的博客","comments":true,"tags":[{"name":"分布式","slug":"分布式","permalink":"http://forvoyager.github.io/tags/分布式/"},{"name":"分布式锁","slug":"分布式锁","permalink":"http://forvoyager.github.io/tags/分布式锁/"}]},{"title":"3、别让手机偷走孩子的注意力","date":"2019-06-17T01:12:52.000Z","path":"2019/06/17/3、别让手机偷走孩子的注意力/","text":"“妈妈，我好无聊，我要看手机。” 每当这个时候，大部分家长都经不住孩子的哼唧，也认为看个十几分钟没多大关系，于是掏出手机，递给孩子，换自己一阵安静。 尤其是在排队时、等吃饭时、写完作业后，这些无聊的碎片时间，孩子更是坐立不安。 但这些被手机侵占的碎片时间，却正在拉开孩子间的距离。 你的做法里，藏着孩子二十年后的未来。 去年春季，一篇名为《别让手机偷走你的梦想》的演讲火爆朋友圈，南京第九中学的张恒柱校长在演讲里，让所有父母和孩子警惕，不要沦为“手机控”。 在今年春季的开学典礼上，张恒柱校长再次指出一个很严肃的事实： “今天，我们的社会已始进入泛娱乐化时代，我们时刻被海量信息包围着，想安静地看会儿书，做几道题，手机来了消息，网页跳出弹窗，不是这个明星绯闻，就是“速看、震惊”这些标题党。 但等你打开，满足好奇心后，看书学习这件事，早已被冲击得支离破碎。” 是的，在这个信息爆炸的时代，只要我们一有时间，注意力都被手机占去了。 孩子长时间浸泡在手机提供的“低思考、高娱乐、低成本、高回报”的感官刺激里，有多少孩子还能够投入到枯燥的、需要不断努力才能看到成果的学习中去？ 然而，我们依旧在允许手机吞噬孩子的注意力。2016年的一个调查数据显示，家长平均每天愿意给孩子使用移动终端的累计时长约为40分钟。 泛娱乐化的时代，孩子的注意力正在被手机偷走。 注意力被偷走的孩子，正在离成功越来越远。 美国心理学家朱利安·斯坦利在1971年启动了一个超常儿童研究项目，在45年时间里跟踪了美国五千名全国排名1%的超常儿童的职业和成就。 研究结果证明，这五千名儿童绝大部分成为了一流的科学家、世界500强的CEO、联邦法官，包括扎克伯格、谢尔盖·布林等人。 研究发现，专注力和整个学习的关联性远高于智商，一个人越专注，他将来获得成就的比例就越高。 泛娱乐化时代，当大家的注意力都被碎片化阅读和爆炸性信息吞噬时，长时间的专注能力，已成为孩子最重要的竞争力之一。 如何提高注意力如何提高孩子的注意力？并不是不让孩子看手机就可以了。 “你给我专心点！” “看这里，别看其他地方！” “你又干嘛去了，快回来写作业！” 哪怕孩子被你压在书桌前，自身专注力不够的孩子，还是一样会走神。 要从根本上提升孩子的注意力，首先要科学地认知注意力。 关于人类注意力的基本规则和普遍情况，澳大利亚的心理学家莉·沃特斯在其著作《优势教养》中给出了这样的一组数据： 儿童，毫无疑问，不擅长集中和保持注意力，多数3岁的孩子只能保持专注3~5分钟。 在6~12岁之间，孩子的注意力会有一个突飞猛进的发展期，孩子的专注时间可以提高到10分钟左右。 在15岁左右，注意力集中能力又会有一个飞跃，这是大脑发展和髓鞘化增多的结果，让我们的注意力可以保持20~25分钟。 此后，我们的注意力水平趋于稳定，所以作为成年人，我们的注意力水平并没有比青少年时高很多。 据沃特斯博士介绍，成年人工作时之所以能够一直专注在电脑或工作台前，并不是我们能连续好几个小时保持专注，而是我们在不断地重新集中注意力，重新保持专注。 回想一下，当我们来到办公室后，打开电脑，专注一段时间后，也会随手喝口水，也会两眼看看远处思考另一个问题，也会突然想到早上忘记提醒孩子带水杯，只是我们又能很快地专注回来罢了。 而我们经常要求孩子把注意力集中在一件事情上，并长时间保持这种状态，不因为别的事情分心。 我们的要求太高了。 这样不是在提高孩子的注意力，而是在强迫他们坐在书桌前而已。 真正提高孩子的注意力，应该是引导孩子自发地十分专注于某一件事，让他增强这个能力。 美国心理学家朱利安· 斯坦利的研究中也指出，学生只有达到在外界看起来十分专注、刻苦、自觉的学习，而自身认为十分自然的时候，学习就进入了有效的深度学习阶段。我们经常看到，孩子在玩乐高积木时一玩就是一个小时，吃饭喝水上厕所都给忘了，这时候孩子就已经进入了深度专注的状态。培养孩子这种自发的注意力，才是关键。 注意力训练场景注意力的形成是一个体系化过程，需要创造一定的环境，以及一定的刻意练习。 但这并不代表只有专业人士才能训练孩子的注意力。只要家长掌握了一定的原理和方法，就可以从生活中找到很多机会去训练孩子的注意力。 尤其是那些被手机侵占的碎片时间，用来做注意力的训练，最合适不过。 等车、排队、学习间歇等碎片时间，跟孩子玩一些有趣的游戏，用孩子喜欢的方式去加强注意力的训练，不知不觉中，孩子的注意力，就“高人一等”了。 等车、坐车等车或坐车时，由于不方便使用道具，所以最好的注意力训练道具就是周围所能看到的一切。 让孩子找车牌号码中的某个数字，找店铺招牌上的某个文字，找这个路口的路牌在哪里，找红绿灯周围有没有摄像头……路上的一切，都可以用来提高孩子的注意力。 平时跟娇娇玩的时候，我就发现，她找出来的数量越多，越兴奋，越想玩，就越专注去找，不知不觉中，就已经进入深度专注。 这个游戏成为我和娇娇等车、坐车的主旋律之后，她的各种无聊、晕车、扭捏，都不见踪影了。 排队、等吃饭1、妈妈背包里有什么：一般出行，为了训练孩子注意力特意拿上道具，没太大必要。妈妈的背包，就是一个很好的道具。让孩子看3-5秒钟背包里有什么物品，然后拉上背包拉链，问孩子背包里头有什么。每玩一次，可以将背包里的某一样物品拿出来之后，再跟孩子玩下一轮。除了说出物品名称之外，还可以让孩子说说物品的颜色，换着法子玩。这个游戏，注意力、观察力、记忆力都能加强，一举三得。 2、接数字：玩这个游戏的孩子一定要达到能流利数数的阶段，特别适合在幼儿园刚刚学了数字的孩子。家长说出几个数字，孩子要接着往下数出一样多的数。例如，家长说1、2、3，孩子就要说4、5、6；家长说7、8，宝宝就要说9、10…这个游戏不仅能训练孩子的专注力，还可以引导孩子学会更多的数字。 3、大小西瓜：这个游戏至少需要3个人，最好拉上爸爸一起玩。第一个人当排头，说“大西瓜”，但两手比成小西瓜的样子；接着第二个人说：“小西瓜”，但两手比成大西瓜的样子，依次直到最后一人。类似的游戏还有，妈妈说“左手”，孩子举起右手；妈妈说“往前一步”，孩子就向后一步。这种类型的游戏，可以破除思维的定式和思维惯性，也是训练孩子专注力的一种方法。 学习间歇、写完作业1、抓手指游戏：妈妈可以随意从书上找一段文字，找到文字里出现频率较高的某一个字。如：弯弯的月儿小小的船。小小的船儿两头尖。我在小小的船里坐，只看见闪闪的星星蓝蓝的天。让孩子把食指放在妈妈的掌心附近，当妈妈念到文字里的“小”字时，妈妈就会抓孩子的食指，孩子听到“小”字时就要快速抽离食指躲避。每次玩这个游戏时，家里总是一片笑声。娇娇长时间学习后紧张的神经，也会因为这个游戏小小的刺激而放松很多，然后又能很好地投入到下一阶段的学习。 2、衣服拍一下、食物拍两下：这个游戏可以结合语文、英语等学习过的词汇来玩。在一年级的英语里，每一个单元学习的都是相同类型物品的词汇，妈妈就可以挑出一组衣服类的单词，和一组食物类的单词，混合在一起。当妈妈念到衣服类的单词时，孩子拍一下掌，当妈妈念到食物类的单词时，孩子拍两下掌。这样既能让孩子在学习间隙放松，又无形中帮助孩子强化所学到的单词。效果杠杠的。 3、6的倍数：这个游戏适合大一点的孩子，跟大人一起玩。一人说一个数，但是轮到带有6的数字，以及6的倍数的数字的人，就不能说话，用拍一下掌代替。这个游戏，非常锻炼孩子的注意力和大脑的运算能力，大人有时候都会算错哦。 法国生物学家乔治.居维叶说：“天才，首先是注意力。” 这个注意力，应该是孩子自发的、自然集中的注意力，而非大人强迫的。 把孩子被手机偷走的那些碎片时间，拿来跟孩子一起玩这些有趣的注意力训练游戏，孩子一定能在游戏中，不知不觉地，玩出一身专注的能力，玩出一个不一样的未来。 来源：娇娇妈（ID: jiaojiaoma8）","comments":true,"tags":[{"name":"未分类","slug":"未分类","permalink":"http://forvoyager.github.io/tags/未分类/"}]},{"title":"2、代码生成器","date":"2019-06-14T09:01:02.000Z","path":"2019/06/14/2、代码生成器/","text":"节省时间去陪老婆，就靠你了…… 代码生成工具用于在项目开发过程中生成mapper, mapper xml, dao, service, controller等基础代码，让开发人员不必花费过多的时间在这些基础代码的开发上。 目前生成的是微服务架构代码格式，如果需要MVC风格代码，忽略掉*-client-starter模块的内容即可。 先贴代码…… 使用方法代码信息配置在CodeGenerator类中按照项目/模块的需要，配置好项目信息、作者信息及数据库信息，然后添加需要生成基础代码的表。 12345678910111213141516171819202122232425// 项目名称String projectName = \"micro_service\";// 基础包名String basePackageName = \"com.xr\";// 模块名称String moduleName = \"account\";// 模块名前缀String modulePrefix = \"ms-\";// 作者String author = \"forvoyager@outlook.com\";// 代码存放路径String outputPath = \"./code\";// 数据库配置String url = \"jdbc:mysql://localhost:3306/ms_account_db?characterEncoding=UTF-8\";String driver = \"com.mysql.jdbc.Driver\";String username = \"root\";String password = \"123456\";// 需要去掉的表前缀String skipTablePrefix = \"ms_\";// 需要生成代码的表List&lt;String&gt; tables = new ArrayList&lt;String&gt;();tables.add(\"ms_account\");tables.add(\"ms_user_level\"); 注： 表及字段信息会从数据库获取，所以建表要规范，相关备注、默认值等配置好。 表名中如果有下划线，会转换为驼峰命名规则，如表：ms_funds_data生成的代码形如FundsDataModel，字段名不受此影响。 生成代码执行com.xr.code.generate.CodeGenerator类，会在指定路径（默认是在当前项目路径下）生成代码文件。 最终生成的代码样例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657micro_service └─ms-account ├─ms-account-client-starter │ └─src │ └─main │ └─java │ └─com │ └─xr │ └─account │ └─client │ AccountClient.java │ UserLevelClient.java │ ├─ms-account-common │ └─src │ └─main │ └─java │ └─com │ └─xr │ └─account │ └─common │ ├─controller │ │ IAccountController.java │ │ IUserLevelController.java │ │ │ └─model │ AccountModel.java │ UserLevelModel.java │ └─ms-account-service └─src └─main ├─java │ └─com │ └─xr │ └─account │ ├─controller │ │ AccountController.java │ │ UserLevelController.java │ │ │ ├─mapper │ │ AccountMapper.java │ │ UserLevelMapper.java │ │ │ └─service │ │ IAccountService.java │ │ IUserLevelService.java │ │ │ └─impl │ AccountServiceImpl.java │ UserLevelServiceImpl.java │ └─resources └─mybatis └─mapper account.xml userLevel.xml 其他一些约定 默认所有表都有如下三个字段 123create_time 创建时间update_time 最后修改时间version 数据版本号（用于乐观锁实现） 所有表的更新操作，版本号自动++ 1UPDATE `table` SET version = version + 1 项目中如下路径的文件是代码模板文件，按需调整。 1code-generator/src/main/resources/tpl 项目中如下路径的文件是mybatis基础配置文件，按需调整。 1code-generator/src/main/resources/mybatis 项目中如下路径的文件是mapper和service的基础文件，按需调整。 1code-generator/src/main/java/com/xr/base/core/service","comments":true,"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://forvoyager.github.io/tags/JAVA/"}]},{"title":"1、github+hexo构建个人博客","date":"2019-06-13T06:12:45.000Z","path":"2019/06/13/1、github-hexo构建个人博客/","text":"第一篇就写你吧…… 安装Node.js具体安装过程此处不赘述，见官方文档，安装完成后执行下面命令验证是否安装成功： 12&gt;node -vv8.7.0 安装Git这个过程就不赘述了，见官方文档，安装完成后执行下面命令验证是否安装成功： 12&gt;git --versiongit version 2.12.2.windows.2 安装Hexo安装按顺序执行下面命令，进行安装： 12345npm install hexo-cli -ghexo init blogcd blognpm installhexo server 注：blog是文件夹名字，根据需要自行修改。 按照上面步骤操作，没有报错，正常安装完成后，访问下面地址，能打开说明安装成功。http://localhost:4000/ blog目录中的文件及目录结构（只列了第一级目录）如下所示： 1234567891011E:\\blog│├─node_modules├─public├─scaffolds├─source├─themes├─config.yml├─db.json├─package.json└─package-lock.json 主题设置默认的主题有点丑，换一个主题吧，以hexo-theme-yilia主题为例，更多主题…… 下载主题操作步骤如下： 123cd blog/themeshexo cleangit clone https://github.com/litten/hexo-theme-yilia.git yilia 修改配置修改blog目录下的_config.yml配置文件。 将theme属性，设置为yilia。 在文件最后添加如下配置，显示文章目录。123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 生成目录： 1npm i hexo-generator-json-content --save 验证主题启动本地web服务器： 1hexo server 访问http://localhost:4000/，查看新主题效果。 写文章文章是markdown文档 创建文章执行命令：1hexo new &quot;文章标题&quot; 然后会在source/_posts路径下生成markdown文件。写文章就是按markdown语法编辑此文件。 如果文章太长，可以在文章中任意你想截断的位置加上如下描述： 1&lt;!-- more --&gt; 文章会在此处截断，文章列表页显示截断之前的内容，点击“查看全文”后显示全部内容。 发布博客创建github账户并配置 注册账号，传送门…… 创建仓库 Repository name必须是这种形式：{username}.github.io 配置hexo 修改blog目录下的_config.yml配置文件中的deploy项为如下形式： 12345deploy: type: git repo: &#123;上面创建的仓库地址，如：https://github.com/username/username.github.io.git&#125; branch: master message: &quot;git commit时的备注信息，自行调整&quot; 发布执行下面命令，发布博客，其实就是生成博客页面，然后上传到github仓库中。 1hexo clean &amp;&amp; hexo deploy 提交过程中会提示输入github用户名和密码。 访问博客传送门","comments":true,"tags":[{"name":"未分类","slug":"未分类","permalink":"http://forvoyager.github.io/tags/未分类/"}]}]